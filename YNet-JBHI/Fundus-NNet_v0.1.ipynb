{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading faiss with AVX2 support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import math\n",
    "import random\n",
    "import heapq \n",
    "import time\n",
    "import copy\n",
    "import itertools  \n",
    "from typing import Dict, List\n",
    "from PIL import Image\n",
    "from io import StringIO,BytesIO \n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.signal import butter, lfilter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize,normalize\n",
    "from sklearn.metrics import confusion_matrix,roc_curve,accuracy_score,auc,roc_auc_score \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.ndimage import zoom\n",
    "from functools import reduce\n",
    "from scipy.io import loadmat\n",
    "from skimage.measure import block_reduce\n",
    "from collections import Counter\n",
    "from scipy.sparse import coo_matrix,hstack, vstack\n",
    "import cv2\n",
    "import faiss \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.ops as ops\n",
    "torch.cuda.set_device(0)\n",
    "print (torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "585 / 585 The length of trainset is 585\n",
      "65 / 65 The length of testset is 65\n",
      "Completed data handle in 96 seconds\n"
     ]
    }
   ],
   "source": [
    "tstart = time.time()\n",
    "root_dir = '/data/fjsdata/MCBIR-Ins/origa650/' #the path of images\n",
    "trData = pd.read_csv(root_dir+\"trainset.csv\" , sep=',')\n",
    "teData = pd.read_csv(root_dir+\"testset.csv\" , sep=',')\n",
    "#trainset \n",
    "trN, trI, trM, trY = [],[],[],[]\n",
    "for iname, itype in np.array(trData).tolist():\n",
    "    iname=os.path.splitext(iname)[0].strip()[1:] #get rid of file extension\n",
    "    try:\n",
    "        trN.append(iname)\n",
    "        if itype==True: #glaucoma\n",
    "            trY.append(1)\n",
    "        else: trY.append(0) #False\n",
    "        image_path = os.path.join(root_dir, 'images', iname+'.jpg')\n",
    "        img = cv2.resize(cv2.imread(image_path).astype(np.float32), (256, 256))#(256,256,3)\n",
    "        trI.append(img)\n",
    "        mask_path = os.path.join(root_dir,'mask', iname+'.mat')\n",
    "        mask = cv2.resize(loadmat(mask_path)['mask'],(256, 256))#(256,256)\n",
    "        trM.append(mask)\n",
    "    except:\n",
    "        print(iname+\":\"+str(image_path))\n",
    "    sys.stdout.write('\\r{} / {} '.format(len(trN),trData.shape[0]))\n",
    "    sys.stdout.flush()\n",
    "print('The length of trainset is %d'%len(trN))\n",
    "#testset\n",
    "teN, teI, teM, teY = [],[],[],[]\n",
    "for iname, itype in np.array(teData).tolist():\n",
    "    iname=os.path.splitext(iname)[0].strip()[1:] #get rid of file extension\n",
    "    try:\n",
    "        teN.append(iname)\n",
    "        if itype==True: #glaucoma\n",
    "            teY.append(1)\n",
    "        else: teY.append(0) #False\n",
    "        image_path = os.path.join(root_dir, 'images', iname+'.jpg')\n",
    "        img = cv2.resize(cv2.imread(image_path).astype(np.float32), (256, 256))#(256,256,3)\n",
    "        teI.append(img)\n",
    "        mask_path = os.path.join(root_dir,'mask', iname+'.mat')\n",
    "        mask = cv2.resize(loadmat(mask_path)['mask'],(256, 256))#(256,256)\n",
    "        teM.append(mask)\n",
    "    except:\n",
    "        print(iname+\":\"+str(image_path))\n",
    "    sys.stdout.write('\\r{} / {} '.format(len(teN),teData.shape[0]))\n",
    "    sys.stdout.flush()\n",
    "print('The length of testset is %d'%len(teN))\n",
    "\n",
    "elapsed = time.time() - tstart\n",
    "print('Completed data handle in %d seconds' % int(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmac_region_coordinates(H, W, L):\n",
    "    # Almost verbatim from Tolias et al Matlab implementation.\n",
    "    # Could be heavily pythonized, but really not worth it...\n",
    "    # Desired overlap of neighboring regions\n",
    "    ovr = 0.4\n",
    "    # Possible regions for the long dimension\n",
    "    steps = np.array((2, 3, 4, 5, 6, 7), dtype=np.float32)\n",
    "    w = np.minimum(H, W)\n",
    "    \n",
    "    b = (np.maximum(H, W) - w) / (steps - 1)\n",
    "    # steps(idx) regions for long dimension. The +1 comes from Matlab\n",
    "    # 1-indexing...\n",
    "    idx = np.argmin(np.abs(((w**2 - w * b) / w**2) - ovr)) + 1\n",
    "    \n",
    "    # Region overplus per dimension\n",
    "    Wd = 0\n",
    "    Hd = 0\n",
    "    if H < W:\n",
    "        Wd = idx\n",
    "    elif H > W:\n",
    "        Hd = idx\n",
    "    \n",
    "    regions_xywh = []\n",
    "    for l in range(1, L+1):\n",
    "        wl = np.floor(2 * w / (l + 1))\n",
    "        wl2 = np.floor(wl / 2 - 1)\n",
    "        # Center coordinates\n",
    "        if l + Wd - 1 > 0:\n",
    "            b = (W - wl) / (l + Wd - 1)\n",
    "        else:\n",
    "            b = 0\n",
    "        cenW = np.floor(wl2 + b * np.arange(l - 1 + Wd + 1)) - wl2\n",
    "        # Center coordinates\n",
    "        if l + Hd - 1 > 0:\n",
    "            b = (H - wl) / (l + Hd - 1)\n",
    "        else:\n",
    "            b = 0\n",
    "        cenH = np.floor(wl2 + b * np.arange(l - 1 + Hd + 1)) - wl2\n",
    "    \n",
    "        for i_ in cenH:\n",
    "            for j_ in cenW:\n",
    "                regions_xywh.append([j_, i_, wl, wl])\n",
    "    \n",
    "    # Round the regions. Careful with the borders!\n",
    "    for i in range(len(regions_xywh)):\n",
    "        for j in range(4):\n",
    "            regions_xywh[i][j] = int(round(regions_xywh[i][j]))\n",
    "        if regions_xywh[i][0] + regions_xywh[i][2] > W:\n",
    "            regions_xywh[i][0] -= ((regions_xywh[i][0] + regions_xywh[i][2]) - W)\n",
    "        if regions_xywh[i][1] + regions_xywh[i][3] > H:\n",
    "            regions_xywh[i][1] -= ((regions_xywh[i][1] + regions_xywh[i][3]) - H)\n",
    "    return np.array(regions_xywh).astype(np.float32)\n",
    "\n",
    "def pack_regions_for_network(all_regions):\n",
    "    n_regs = np.sum([len(e) for e in all_regions])\n",
    "    R = np.zeros((n_regs, 5), dtype=np.float32)\n",
    "    cnt = 0\n",
    "    # There should be a check of overflow...\n",
    "    for i, r in enumerate(all_regions):\n",
    "        try:\n",
    "            R[cnt:cnt + r.shape[0], 0] = i\n",
    "            R[cnt:cnt + r.shape[0], 1:] = r\n",
    "            cnt += r.shape[0]\n",
    "        except:\n",
    "            continue\n",
    "    assert cnt == n_regs\n",
    "    R = R[:n_regs]\n",
    "    # regs where in xywh format. R is in xyxy format, where the last coordinate is included. Therefore...\n",
    "    R[:n_regs, 3] = R[:n_regs, 1] + R[:n_regs, 3] - 1\n",
    "    R[:n_regs, 4] = R[:n_regs, 2] + R[:n_regs, 4] - 1\n",
    "    return R\n",
    "\n",
    "class L2Normalization(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we construct three nn.Linear instances that we will use\n",
    "        in the forward pass.\n",
    "        \"\"\"\n",
    "        super(L2Normalization, self).__init__()\n",
    "        self.eps = 1e-8\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if x.is_cuda:\n",
    "            caped_eps = Variable(torch.Tensor([self.eps])).cuda(torch.cuda.device_of(x).idx)\n",
    "        else:\n",
    "            caped_eps = Variable(torch.Tensor([self.eps]))\n",
    "        x = torch.div(x.transpose(0,1),x.max(1)[0]).transpose(0,1) # max_normed\n",
    "        norm = torch.norm(x,2,1) + caped_eps.expand(x.size()[0])\n",
    "        y = torch.div(x.transpose(0,1),norm).transpose(0,1)\n",
    "        return y\n",
    "    \n",
    "class RoIPool(nn.Module):\n",
    "    def __init__(self, pooled_height, pooled_width, spatial_scale):\n",
    "        super(RoIPool, self).__init__()\n",
    "        self.pooled_width = int(pooled_width)\n",
    "        self.pooled_height = int(pooled_height)\n",
    "        self.spatial_scale = float(spatial_scale)\n",
    "\n",
    "    def forward(self, features, rois):\n",
    "        batch_size, num_channels, data_height, data_width = features.size()\n",
    "        num_rois = rois.size()[0]\n",
    "        \n",
    "        outputs = Variable(torch.zeros(num_rois, num_channels,\n",
    "                                       self.pooled_height,\n",
    "                                       self.pooled_width))\n",
    "        if features.is_cuda:\n",
    "            outputs = outputs.cuda(torch.cuda.device_of(features).idx)\n",
    "\n",
    "        for roi_ind, roi in enumerate(rois):\n",
    "            batch_ind = int(roi[0].item())\n",
    "            roi_start_w, roi_start_h, roi_end_w, roi_end_h = torch.round(roi[1:]* self.spatial_scale).data.cpu().numpy().astype(int)\n",
    "            roi_width = max(roi_end_w - roi_start_w + 1, 1)\n",
    "            roi_height = max(roi_end_h - roi_start_h + 1, 1)\n",
    "            bin_size_w = float(roi_width) / float(self.pooled_width)\n",
    "            bin_size_h = float(roi_height) / float(self.pooled_height)\n",
    "\n",
    "            for ph in range(self.pooled_height):\n",
    "                hstart = int(np.floor(ph * bin_size_h))\n",
    "                hend = int(np.ceil((ph + 1) * bin_size_h))\n",
    "                hstart = min(data_height, max(0, hstart + roi_start_h))\n",
    "                hend = min(data_height, max(0, hend + roi_start_h))\n",
    "                for pw in range(self.pooled_width):\n",
    "                    wstart = int(np.floor(pw * bin_size_w))\n",
    "                    wend = int(np.ceil((pw + 1) * bin_size_w))\n",
    "                    wstart = min(data_width, max(0, wstart + roi_start_w))\n",
    "                    wend = min(data_width, max(0, wend + roi_start_w))\n",
    "\n",
    "                    is_empty = (hend <= hstart) or(wend <= wstart)\n",
    "                    if is_empty:\n",
    "                        outputs[roi_ind, :, ph, pw] = 0\n",
    "                    else:\n",
    "                        data = features[batch_ind]\n",
    "                        outputs[roi_ind, :, ph, pw] = torch.max(\n",
    "                            torch.max(data[:, hstart:hend, wstart:wend], 1, keepdim = True)[0], 2, keepdim = True)[0].view(-1)  # noqa\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "class ContextAwareRegionalAttentionNetwork(nn.Module):\n",
    "    def __init__(self, spatial_scale, pooled_height = 1, pooled_width = 1):\n",
    "        super(ContextAwareRegionalAttentionNetwork, self).__init__()\n",
    "        self.pooled_width = int(pooled_width)\n",
    "        self.pooled_height = int(pooled_height)\n",
    "        self.spatial_scale = float(spatial_scale)\n",
    "        \n",
    "        self.conv_att_1 = nn.Conv1d(1024, 64, 1, padding=0)\n",
    "        self.sp_att_1 = nn.Softplus()\n",
    "        self.conv_att_2 = nn.Conv1d(64, 1, 1, padding=0)\n",
    "        self.sp_att_2 = nn.Softplus()\n",
    "        \n",
    "\n",
    "    def forward(self, features, rois):\n",
    "        batch_size, num_channels, data_height, data_width = features.size()\n",
    "        num_rois = rois.size()[0]\n",
    "        \n",
    "        outputs = Variable(torch.zeros(num_rois, num_channels*2,\n",
    "                                       self.pooled_height,\n",
    "                                       self.pooled_width))\n",
    "        if features.is_cuda:\n",
    "            outputs = outputs.cuda(torch.cuda.device_of(features).idx)\n",
    "            \n",
    "        # Based on roi pooling code of pytorch but, the only difference is to change max pooling to mean pooling\n",
    "        for roi_ind, roi in enumerate(rois):\n",
    "            batch_ind = int(roi[0].item())\n",
    "            roi_start_w, roi_start_h, roi_end_w, roi_end_h =  torch.round(roi[1:]* self.spatial_scale).data.cpu().numpy().astype(int)\n",
    "            roi_width = max(roi_end_w - roi_start_w + 1, 1)\n",
    "            roi_height = max(roi_end_h - roi_start_h + 1, 1)\n",
    "            bin_size_w = float(roi_width) / float(self.pooled_width)\n",
    "            bin_size_h = float(roi_height) / float(self.pooled_height)\n",
    "\n",
    "            for ph in range(self.pooled_height):\n",
    "                hstart = int(np.floor(ph * bin_size_h))\n",
    "                hend = int(np.ceil((ph + 1) * bin_size_h))\n",
    "                hstart = min(data_height, max(0, hstart + roi_start_h))\n",
    "                hend = min(data_height, max(0, hend + roi_start_h))\n",
    "                for pw in range(self.pooled_width):\n",
    "                    wstart = int(np.floor(pw * bin_size_w))\n",
    "                    wend = int(np.ceil((pw + 1) * bin_size_w))\n",
    "                    wstart = min(data_width, max(0, wstart + roi_start_w))\n",
    "                    wend = min(data_width, max(0, wend + roi_start_w))\n",
    "\n",
    "                    is_empty = (hend <= hstart) or(wend <= wstart)\n",
    "                    if is_empty:\n",
    "                        outputs[roi_ind, :, ph, pw] = 0\n",
    "                    else:\n",
    "                        data = features[batch_ind]\n",
    "                        # mean pooling with both of regional feature map and global feature map\n",
    "                        outputs[roi_ind, :, ph, pw] = torch.cat((torch.mean(\n",
    "                            torch.mean(data[:, hstart:hend, wstart:wend], 1, keepdim = True), 2, keepdim = True).view(-1)\n",
    "                            ,torch.mean(\n",
    "                            torch.mean(data, 1, keepdim = True), 2, keepdim = True).view(-1)), 0 )  # noqa\n",
    "                        \n",
    "        # Reshpae\n",
    "        outputs = outputs.squeeze(2).squeeze(2)\n",
    "        outputs = outputs.transpose(0,1).unsqueeze(0) # (1, # channel, #batch * # regions)\n",
    "        #Calculate regional attention weights with context-aware regional feature vectors\n",
    "        k = self.sp_att_1(self.conv_att_1(outputs))\n",
    "        k = self.sp_att_2(self.conv_att_2(k)) # (1, 1, #batch * # regions)\n",
    "        k = torch.squeeze(k,1)\n",
    "        \n",
    "        return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/luyajie/triplet-deep-hash-pytorch#triplet-deep-hash-pytorch            \n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=0.5):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin #margin threshold\n",
    "        self.mse_loss = nn.MSELoss(reduction='none')\n",
    "    \n",
    "    def forward(self,H_q,H_p,H_n):    \n",
    "        margin_val = self.margin * H_q.shape[1]\n",
    "        squared_loss_pos = torch.mean(self.mse_loss(H_q, H_p), dim=1)\n",
    "        squared_loss_neg = torch.mean(self.mse_loss(H_q, H_n), dim=1)\n",
    "        zeros = torch.zeros_like(squared_loss_neg)\n",
    "        loss  = torch.max(zeros, margin_val - squared_loss_neg + squared_loss_pos)\n",
    "        return torch.mean(loss)\n",
    "    \n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "    \n",
    "class NNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks=[2,2,2,2], n_classes=2, code_size=64):\n",
    "        super(NNet, self).__init__()\n",
    "        # Bottom-up layersï¼Œclassifcation loss\n",
    "        self.in_planes = 8  #3 D->64 channels\n",
    "        self.conv1 = nn.Conv2d(3, 8, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "\n",
    "        self.layer2 = self._make_layer(block, 8, num_blocks[0], stride=1)\n",
    "        self.layer3 = self._make_layer(block, 16, num_blocks[1], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 32, num_blocks[2], stride=2)\n",
    "        self.layer5 = self._make_layer(block, 64, num_blocks[3], stride=2)\n",
    "        self.conv6 = nn.Conv2d(256, 32, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(32*4*4,code_size)#code_size:length of hash code\n",
    "        self.fc2 = nn.Linear(code_size,n_classes) #num_classes:number of classes\n",
    "\n",
    "        # Top-down layer, segmentation loss\n",
    "        self.toplayer = nn.Conv2d(256, 32, kernel_size=1, stride=1, padding=0)  # Reduce channels\n",
    "        \n",
    "        self.latlayer1 = nn.Conv2d(128, 32, kernel_size=1, stride=1, padding=0)# Lateral layers\n",
    "        self.latlayer2 = nn.Conv2d(64, 32, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        self.upsample = nn.Upsample((256,256), mode='bilinear',align_corners=True)\n",
    "        self.conv7 = nn.Conv2d(32, 3, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(3)#mask 0,1,2\n",
    "        \n",
    "        # Hash layer, ranking loss\n",
    "        self.conv8 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv9 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv10 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv11 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n",
    "        self.r_mac_pool = RoIPool(1,1,0.03125)#RoI max pooling\n",
    "        self.region_attention = ContextAwareRegionalAttentionNetwork(spatial_scale = 0.03125)\n",
    "        self.l2norm = L2Normalization()\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _upsample_add(self, x, y):\n",
    "        '''Upsample and add two feature maps.\n",
    "        Args:\n",
    "          x: (Variable) top feature map to be upsampled.\n",
    "          y: (Variable) lateral feature map.\n",
    "        Returns:\n",
    "          (Variable) added feature map.\n",
    "        Note in PyTorch, when input size is odd, the upsampled feature map\n",
    "        with `F.upsample(..., scale_factor=2, mode='nearest')`\n",
    "        maybe not equal to the lateral feature map size.\n",
    "        e.g.\n",
    "        original input size: [N,_,15,15] ->\n",
    "        conv2d feature map size: [N,_,8,8] ->\n",
    "        upsampled feature map size: [N,_,16,16]\n",
    "        So we choose bilinear upsample which supports arbitrary output sizes.\n",
    "        '''\n",
    "        _,_,H,W = y.size()\n",
    "        return F.upsample(x, size=(H,W), mode='bilinear') + y\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Bottom-up, classifcation loss\n",
    "        c1 = F.relu(self.bn1(self.conv1(x)))#(3,256,256)->(8,128,128)\n",
    "        c1 = F.max_pool2d(c1, kernel_size=3, stride=2, padding=1)#(8,128,128)->(8,64,64)\n",
    "        \n",
    "        c2 = self.layer2(c1)#(8,64,64)->(32,64,64)\n",
    "        c3 = self.layer3(c2)#(32,64,64)->(64,32,32)\n",
    "        c4 = self.layer4(c3)#(64,32,32)->(128,16,16)\n",
    "        c5 = self.layer5(c4)#(128,16,16)->(256,8,8)\n",
    "        \n",
    "        c6 = self.conv6(c5)#(256,8,8)->(32,4,4) \n",
    "        c6 = c6.view(c6.size(0), -1)#conv->linear\n",
    "        c_hash = self.fc1(c6)\n",
    "        c_cls  = self.fc2(c_hash)\n",
    "        \n",
    "        # Top-down, segmentation loss\n",
    "        s5 = self.toplayer(c5)#(256,8,8)->(32,8,8)\n",
    "        s4 = self._upsample_add(s5, self.latlayer1(c4))#{(32,8,8),(32, 16, 16)}->(32, 16, 16)\n",
    "        s3 = self._upsample_add(s4, self.latlayer2(c3))#{(32, 16, 16),(32, 32, 32)}->(32, 32, 32)\n",
    "        s2 = self._upsample_add(s3, c2) #{(32, 32, 32),(32, 64, 64)}->(32, 64, 64)\n",
    "        \n",
    "        s1 = self.upsample(s2)#(32, 64, 64)->(32, 256, 256)\n",
    "        s_mask = self.bn2(self.conv7(s1))#(32, 256, 256)->(3, 256, 256)\n",
    "        \n",
    "        #Hash, ranking loss\n",
    "        #Calculate R-MAC regions (Region sampling)\n",
    "        batched_rois =  [get_rmac_region_coordinates(s1.shape[2],s1.shape[3],5) for i in range(s1.shape[0])]\n",
    "        rois = Variable(torch.FloatTensor(pack_regions_for_network(batched_rois)))\n",
    "        \n",
    "        h1 = self.conv8(s1)#(32, 256, 256)->(64, 128, 128)\n",
    "        h2 = self.conv9(h1)#(64, 128, 128)->(128, 64, 64)\n",
    "        h3 = self.conv10(h2)#(128, 64, 64)->(256, 32, 32)\n",
    "        h4 = self.conv11(h3)#(256, 32, 32)->(512, 16, 16)\n",
    "        \n",
    "        h5 = self.r_mac_pool(h4,rois) \n",
    "        h5 = h5.squeeze(2).squeeze(2) # (batch * regions, channel)\n",
    "        h5_att = self.region_attention(h4,rois)\n",
    "        h5_att = h5_att.squeeze(0).squeeze(0)# (# batch * region)\n",
    "        \n",
    "        #weighted mean\n",
    "        h6 = torch.mul(h5.transpose(1,0),h5_att).transpose(1,0)  # regional weighted feature (# batch * region, #channel)\n",
    "        h6 = h6.contiguous()\n",
    "        h6 = h6.view(torch.Size([h4.size(0), -1 ,h4.size(1)])) # (#batch, # region, # channel)\n",
    "        h6 = torch.transpose(h6,1,2)    # (#batch * #channel, #region)\n",
    "        \n",
    "        h6 = torch.mean(h6,2) #mean\n",
    "        h_hash = self.l2norm(h6)\n",
    "\n",
    "        return c_hash,c_cls,s_mask, h_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0507, 0.0265, 0.0326,  ..., 0.0540, 0.0378, 0.0371],\n",
      "        [0.0286, 0.0331, 0.0296,  ..., 0.0311, 0.0388, 0.0309],\n",
      "        [0.0449, 0.0378, 0.0277,  ..., 0.0435, 0.0428, 0.0316],\n",
      "        ...,\n",
      "        [0.0426, 0.0268, 0.0270,  ..., 0.0331, 0.0359, 0.0264],\n",
      "        [0.0438, 0.0237, 0.0333,  ..., 0.0395, 0.0493, 0.0286],\n",
      "        [0.0307, 0.0228, 0.0239,  ..., 0.0385, 0.0352, 0.0278]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "torch.Size([10, 64])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 512])\n"
     ]
    }
   ],
   "source": [
    "net = NNet(block=Bottleneck)\n",
    "fms = net(Variable(torch.randn(10,3,256,256)))\n",
    "for fm in fms:\n",
    "    print(fm.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 59 / 59 : loss = 4.797076Eopch:     1 mean_loss = 5.176730\n",
      " 59 / 59 : loss = 3.839346Eopch:     2 mean_loss = 4.597251\n",
      " 59 / 59 : loss = 3.815613Eopch:     3 mean_loss = 4.022905\n",
      " 59 / 59 : loss = 2.990834Eopch:     4 mean_loss = 3.339956\n",
      " 59 / 59 : loss = 4.112644Eopch:     5 mean_loss = 3.126902\n",
      " 59 / 59 : loss = 4.163306Eopch:     6 mean_loss = 2.706744\n",
      " 59 / 59 : loss = 2.098885Eopch:     7 mean_loss = 2.344119\n",
      " 59 / 59 : loss = 1.983567Eopch:     8 mean_loss = 2.318032\n",
      " 59 / 59 : loss = 1.874482Eopch:     9 mean_loss = 1.982428\n",
      " 59 / 59 : loss = 1.619328Eopch:    10 mean_loss = 1.888633\n",
      "best_loss = 1.888633\n",
      " 6 / 7 9 Completed buliding index in 1 seconds\n",
      "mAP=0.5571, mIoU=0.7188\n"
     ]
    }
   ],
   "source": [
    "#Generate image pairs for model\n",
    "def onlineGenImgPairs( ):\n",
    "    idx_sf = []\n",
    "    idx_0 = np.where( np.array(trY) == 0 ) #class 0\n",
    "    idx_0 = list(idx_0[0])\n",
    "    idx_sf.extend(idx_0)\n",
    "    idx_1 = np.where( np.array(trY) == 1 ) #class 1\n",
    "    idx_1 = list(idx_1[0])\n",
    "    idx_sf.extend(idx_1)\n",
    "    random.shuffle(idx_sf)   \n",
    "    trQ_sf, trP_sf, trN_sf = [], [], []\n",
    "    trQ_y, trP_y, trN_y = [], [], []\n",
    "    trQ_m, trP_m, trN_m = [], [], []\n",
    "    for iQ in idx_sf:\n",
    "        trQ_sf.append(trI[iQ])\n",
    "        trQ_y.append(trY[iQ])\n",
    "        trQ_m.append(trM[iQ])\n",
    "        if trY[iQ] == 0:\n",
    "            idx_tmp = idx_0.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            trP_y.append(trY[iP[0]])\n",
    "            trP_m.append(trM[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_0))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "            trN_y.append(trY[iN[0]])\n",
    "            trN_m.append(trM[iN[0]])\n",
    "        elif trY[iQ] == 1:\n",
    "            idx_tmp = idx_1.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            trP_y.append(trY[iP[0]])\n",
    "            trP_m.append(trM[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_1))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "            trN_y.append(trY[iN[0]])\n",
    "            trN_m.append(trM[iN[0]])\n",
    "        else: pass\n",
    "        sys.stdout.write('\\r{} / {} '.format(len(trQ_sf),len(idx_sf)))\n",
    "        sys.stdout.flush()\n",
    "    return np.array(trQ_sf),np.array(trP_sf),np.array(trN_sf), np.array(trQ_y), np.array(trP_y), np.array(trN_y), np.array(trQ_m), np.array(trP_m), np.array(trN_m)\n",
    "#sample  triplet labels\n",
    "trQ_sf, trP_sf, trN_sf, trQ_y, trP_y, trN_y, trQ_m, trP_m, trN_m = onlineGenImgPairs() \n",
    "assert (trQ_sf.shape==trP_sf.shape and trQ_sf.shape==trN_sf.shape)\n",
    "assert (trQ_y.shape==trP_y.shape and trQ_y.shape==trN_y.shape)\n",
    "assert (trQ_m.shape==trP_m.shape and trQ_m.shape==trN_m.shape)\n",
    "assert (np.mean(np.where((np.array(trQ_y)-np.array(trP_y))!=0,1,0))==0.0)\n",
    "assert (np.mean(np.where((np.array(trQ_y)-np.array(trN_y))!=0,1,0))==1.0)\n",
    "\n",
    "#define model\n",
    "model = NNet(block=Bottleneck).cuda()#initialize model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "tl_loss  = TripletLoss(margin=0.5).cuda() #define TripletLoss \n",
    "ce_loss  = nn.CrossEntropyLoss().cuda() #define ce mutli-classes #F.log_softmax+F.nll_loss\n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "for epoch in range(10):#iteration\n",
    "    losses = []\n",
    "    shuffled_idx = np.random.permutation(np.arange(len(trQ_sf)))\n",
    "    train_q = trQ_sf[shuffled_idx]\n",
    "    train_q_y = trQ_y[shuffled_idx]\n",
    "    train_q_m = trQ_m[shuffled_idx]\n",
    "    train_p = trP_sf[shuffled_idx]\n",
    "    train_p_y = trP_y[shuffled_idx]\n",
    "    train_p_m = trP_m[shuffled_idx]\n",
    "    train_n = trN_sf[shuffled_idx]\n",
    "    train_n_y = trN_y[shuffled_idx]\n",
    "    train_n_m = trN_m[shuffled_idx]\n",
    "    num_batches = len(trQ_sf) // batchSize + 1\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trQ_sf), (i+1)*batchSize])\n",
    "        Q_batch = torch.from_numpy(train_q[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        Q_y_batch = torch.from_numpy(train_q_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        Q_m_batch = torch.from_numpy(train_q_m[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        P_batch = torch.from_numpy(train_p[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        P_y_batch = torch.from_numpy(train_p_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        P_m_batch = torch.from_numpy(train_p_m[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        N_batch = torch.from_numpy(train_n[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        N_y_batch = torch.from_numpy(train_n_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        N_m_batch = torch.from_numpy(train_n_m[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        #forword\n",
    "        _, Q_cls, Q_mask, Q_hash = model(Q_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        _, P_cls, P_mask, P_hash = model(P_batch.permute(0, 3, 1, 2))\n",
    "        _, N_cls, N_mask, N_hash = model(N_batch.permute(0, 3, 1, 2))\n",
    "        #loss\n",
    "        cls_loss = ce_loss(Q_cls,Q_y_batch) + ce_loss(P_cls,P_y_batch) + ce_loss(N_cls,N_y_batch)\n",
    "        cls_loss.backward(retain_graph=True) #buffer\n",
    "        mask_loss = ce_loss(Q_mask,Q_m_batch) + ce_loss(P_mask,P_m_batch) + ce_loss(N_mask,N_m_batch)\n",
    "        mask_loss.backward()\n",
    "        #rank_loss = tl_loss(Q_hash,P_hash,N_hash)\n",
    "        #rank_loss.backward()\n",
    "        optimizer.step()#update parameters\n",
    "        #show loss\n",
    "        loss = cls_loss + mask_loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "model = model.cpu()#release gpu memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#hash code of train data \n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize +1\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    x_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    _, _, _, x_hash = best_net(x_batch.permute(0, 3, 1, 2))#forword\n",
    "    trF.extend(x_hash.cpu().data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data\n",
    "teF = [] \n",
    "num_batches = len(teY) // batchSize  +1\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teY), (i+1)*batchSize])\n",
    "    x_batch = torch.from_numpy(np.array(teI[min_idx:max_idx])).type(torch.FloatTensor).cuda()\n",
    "    _, _, _, x_hash= best_net(x_batch.permute(0, 3, 1, 2))#forword\n",
    "    teF.extend(x_hash.cpu().data.numpy().tolist()) #record feature\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#evaluate\n",
    "#compute the size of lesion\n",
    "def Func_IOU_size(pred,target,n_classes = 3 ):\n",
    "    ious = []\n",
    "    # ignore IOU for background class\n",
    "    for cls in range(1,n_classes):\n",
    "        pred_inds = pred == cls\n",
    "        pred_sum = pred_inds.sum()\n",
    "        target_inds = target == cls\n",
    "        target_sum = target_inds.sum()\n",
    "        ious.append(round(float(min(pred_sum,target_sum)/max(pred_sum,target_sum)),4))\n",
    "    return np.mean(ious)\n",
    "# buliding index of trainset\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(512) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(trF, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "for topk in [10]:\n",
    "    mAP = [] #mean average precision\n",
    "    mIoU = []\n",
    "    scores, neighbors = gpu_index.search(np.array(teF).astype('float32'), k=topk)\n",
    "    for i, teVal in enumerate(teF):\n",
    "        stype = teY[i]\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        #for j in ranklist:\n",
    "        for j in neighbors[i].tolist():\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                pos_len = pos_len +1\n",
    "                mAP.append(pos_len/rank_len) \n",
    "            else: \n",
    "                mAP.append(0)\n",
    "            mIoU.append(Func_IOU_size(teM[i],trM[j]))\n",
    "    print(\"mAP={:.4f}, mIoU={:.4f}\".format(np.mean(mAP),np.mean(mIoU)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#release gpu memory\n",
    "model = model.cpu()\n",
    "tl_loss=tl_loss.cpu()\n",
    "ce_loss=ce_loss.cpu()\n",
    "Q_batch = Q_batch.cpu()\n",
    "Q_y_batch = Q_y_batch.cpu()\n",
    "Q_m_batch = Q_m_batch.cpu()\n",
    "P_batch = P_batch.cpu()\n",
    "P_y_batch = P_y_batch.cpu()\n",
    "P_m_batch = P_m_batch.cpu()\n",
    "N_batch = N_batch.cpu()\n",
    "N_y_batch = N_y_batch.cpu()\n",
    "N_m_batch = N_m_batch.cpu()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=======================================demo==================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 59 / 59 : loss = 4.087403Eopch:     1 mean_loss = 5.046442\n",
      " 59 / 59 : loss = 3.561321Eopch:     2 mean_loss = 4.248515\n",
      " 59 / 59 : loss = 4.488179Eopch:     3 mean_loss = 3.512605\n",
      " 59 / 59 : loss = 2.796076Eopch:     4 mean_loss = 3.155894\n",
      " 59 / 59 : loss = 2.292304Eopch:     5 mean_loss = 2.655809\n",
      " 59 / 59 : loss = 3.068572Eopch:     6 mean_loss = 2.429540\n",
      " 59 / 59 : loss = 2.213996Eopch:     7 mean_loss = 2.299044\n",
      " 59 / 59 : loss = 1.835217Eopch:     8 mean_loss = 2.068977\n",
      " 59 / 59 : loss = 3.616864Eopch:     9 mean_loss = 2.008943\n",
      " 59 / 59 : loss = 1.566122Eopch:    10 mean_loss = 1.892587\n",
      " 59 / 59 : loss = 1.509367Eopch:    11 mean_loss = 1.562429\n",
      " 59 / 59 : loss = 1.872504Eopch:    12 mean_loss = 1.490831\n",
      " 59 / 59 : loss = 1.303109Eopch:    13 mean_loss = 1.578016\n",
      " 59 / 59 : loss = 1.291993Eopch:    14 mean_loss = 1.353199\n",
      " 59 / 59 : loss = 1.118256Eopch:    15 mean_loss = 1.252404\n",
      " 59 / 59 : loss = 1.024329Eopch:    16 mean_loss = 1.217060\n",
      " 59 / 59 : loss = 0.954425Eopch:    17 mean_loss = 1.026905\n",
      " 59 / 59 : loss = 0.914608Eopch:    18 mean_loss = 1.029631\n",
      " 59 / 59 : loss = 0.943399Eopch:    19 mean_loss = 0.952479\n",
      " 59 / 59 : loss = 0.807353Eopch:    20 mean_loss = 0.892261\n",
      "best_loss = 0.892261\n",
      " 6 / 7 9 Completed buliding index in 1 seconds\n",
      "mAP=0.7166, mIoU=0.7297\n"
     ]
    }
   ],
   "source": [
    "#NNET-N1:  backward separately\n",
    "#Generate image pairs for model\n",
    "def onlineGenImgPairs( ):\n",
    "    idx_sf = []\n",
    "    idx_0 = np.where( np.array(trY) == 0 ) #class 0\n",
    "    idx_0 = list(idx_0[0])\n",
    "    idx_sf.extend(idx_0)\n",
    "    idx_1 = np.where( np.array(trY) == 1 ) #class 1\n",
    "    idx_1 = list(idx_1[0])\n",
    "    idx_sf.extend(idx_1)\n",
    "    random.shuffle(idx_sf)   \n",
    "    trQ_sf, trP_sf, trN_sf = [], [], []\n",
    "    trQ_y, trP_y, trN_y = [], [], []\n",
    "    trQ_m, trP_m, trN_m = [], [], []\n",
    "    for iQ in idx_sf:\n",
    "        trQ_sf.append(trI[iQ])\n",
    "        trQ_y.append(trY[iQ])\n",
    "        trQ_m.append(trM[iQ])\n",
    "        if trY[iQ] == 0:\n",
    "            idx_tmp = idx_0.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            trP_y.append(trY[iP[0]])\n",
    "            trP_m.append(trM[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_0))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "            trN_y.append(trY[iN[0]])\n",
    "            trN_m.append(trM[iN[0]])\n",
    "        elif trY[iQ] == 1:\n",
    "            idx_tmp = idx_1.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            trP_y.append(trY[iP[0]])\n",
    "            trP_m.append(trM[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_1))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "            trN_y.append(trY[iN[0]])\n",
    "            trN_m.append(trM[iN[0]])\n",
    "        else: pass\n",
    "        sys.stdout.write('\\r{} / {} '.format(len(trQ_sf),len(idx_sf)))\n",
    "        sys.stdout.flush()\n",
    "    return np.array(trQ_sf),np.array(trP_sf),np.array(trN_sf), np.array(trQ_y), np.array(trP_y), np.array(trN_y), np.array(trQ_m), np.array(trP_m), np.array(trN_m)\n",
    "#sample  triplet labels\n",
    "trQ_sf, trP_sf, trN_sf, trQ_y, trP_y, trN_y, trQ_m, trP_m, trN_m = onlineGenImgPairs() \n",
    "assert (trQ_sf.shape==trP_sf.shape and trQ_sf.shape==trN_sf.shape)\n",
    "assert (trQ_y.shape==trP_y.shape and trQ_y.shape==trN_y.shape)\n",
    "assert (trQ_m.shape==trP_m.shape and trQ_m.shape==trN_m.shape)\n",
    "assert (np.mean(np.where((np.array(trQ_y)-np.array(trP_y))!=0,1,0))==0.0)\n",
    "assert (np.mean(np.where((np.array(trQ_y)-np.array(trN_y))!=0,1,0))==1.0)\n",
    "\n",
    "#define model\n",
    "model = NNet(block=Bottleneck).cuda()#initialize model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "tl_loss  = TripletLoss(margin=0.5).cuda() #define TripletLoss \n",
    "ce_loss  = nn.CrossEntropyLoss().cuda() #define ce mutli-classes #F.log_softmax+F.nll_loss\n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "for epoch in range(20):#iteration\n",
    "    losses = []\n",
    "    shuffled_idx = np.random.permutation(np.arange(len(trQ_sf)))\n",
    "    train_q = trQ_sf[shuffled_idx]\n",
    "    train_q_y = trQ_y[shuffled_idx]\n",
    "    train_q_m = trQ_m[shuffled_idx]\n",
    "    train_p = trP_sf[shuffled_idx]\n",
    "    train_p_y = trP_y[shuffled_idx]\n",
    "    train_p_m = trP_m[shuffled_idx]\n",
    "    train_n = trN_sf[shuffled_idx]\n",
    "    train_n_y = trN_y[shuffled_idx]\n",
    "    train_n_m = trN_m[shuffled_idx]\n",
    "    num_batches = len(trQ_sf) // batchSize + 1\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trQ_sf), (i+1)*batchSize])\n",
    "        Q_batch = torch.from_numpy(train_q[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        Q_y_batch = torch.from_numpy(train_q_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        Q_m_batch = torch.from_numpy(train_q_m[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        P_batch = torch.from_numpy(train_p[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        P_y_batch = torch.from_numpy(train_p_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        P_m_batch = torch.from_numpy(train_p_m[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        N_batch = torch.from_numpy(train_n[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        N_y_batch = torch.from_numpy(train_n_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        N_m_batch = torch.from_numpy(train_n_m[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        #forword\n",
    "        Q_hash, Q_cls, Q_mask = model(Q_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        P_hash, P_cls, P_mask = model(P_batch.permute(0, 3, 1, 2))\n",
    "        N_hash, N_cls, N_mask = model(N_batch.permute(0, 3, 1, 2))\n",
    "        #loss\n",
    "        #hash_loss = tl_loss(Q_hash,P_hash,N_hash)\n",
    "        cls_loss = ce_loss(Q_cls,Q_y_batch) + ce_loss(P_cls,P_y_batch) + ce_loss(N_cls,N_y_batch)\n",
    "        cls_loss.backward(retain_graph=True) #buffer\n",
    "        mask_loss = ce_loss(Q_mask,Q_m_batch) + ce_loss(P_mask,P_m_batch) + ce_loss(N_mask,N_m_batch)\n",
    "        mask_loss.backward()\n",
    "        optimizer.step()#update parameters\n",
    "        #show loss\n",
    "        loss = cls_loss + mask_loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "model = model.cpu()#release gpu memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#hash code of train data \n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize +1\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    x_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    x_hash, _, _ = best_net(x_batch.permute(0, 3, 1, 2))#forword\n",
    "    trF.extend(x_hash.cpu().data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data\n",
    "teF = [] \n",
    "num_batches = len(teY) // batchSize  +1\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teY), (i+1)*batchSize])\n",
    "    x_batch = torch.from_numpy(np.array(teI[min_idx:max_idx])).type(torch.FloatTensor).cuda()\n",
    "    x_hash, _, _ = best_net(x_batch.permute(0, 3, 1, 2))#forword\n",
    "    teF.extend(x_hash.cpu().data.numpy().tolist()) #record feature\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#evaluate\n",
    "#compute the size of lesion\n",
    "def Func_IOU_size(pred,target,n_classes = 3 ):\n",
    "    ious = []\n",
    "    # ignore IOU for background class\n",
    "    for cls in range(1,n_classes):\n",
    "        pred_inds = pred == cls\n",
    "        pred_sum = pred_inds.sum()\n",
    "        target_inds = target == cls\n",
    "        target_sum = target_inds.sum()\n",
    "        ious.append(round(float(min(pred_sum,target_sum)/max(pred_sum,target_sum)),4))\n",
    "    return np.mean(ious)\n",
    "# buliding index of trainset\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(64) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(trF, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "for topk in [10]:\n",
    "    mAP = [] #mean average precision\n",
    "    mIoU = []\n",
    "    scores, neighbors = gpu_index.search(np.array(teF).astype('float32'), k=topk)\n",
    "    for i, teVal in enumerate(teF):\n",
    "        stype = teY[i]\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        #for j in ranklist:\n",
    "        for j in neighbors[i].tolist():\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                pos_len = pos_len +1\n",
    "                mAP.append(pos_len/rank_len) \n",
    "            else: \n",
    "                mAP.append(0)\n",
    "            mIoU.append(Func_IOU_size(teM[i],trM[j]))\n",
    "    print(\"mAP={:.4f}, mIoU={:.4f}\".format(np.mean(mAP),np.mean(mIoU)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 59 / 59 : loss = 4.817265Eopch:     1 mean_loss = 5.109196\n",
      " 59 / 59 : loss = 4.139876Eopch:     2 mean_loss = 4.476645\n",
      " 59 / 59 : loss = 4.541694Eopch:     3 mean_loss = 3.997607\n",
      " 59 / 59 : loss = 3.277998Eopch:     4 mean_loss = 3.412922\n",
      " 59 / 59 : loss = 3.330598Eopch:     5 mean_loss = 2.937641\n",
      " 59 / 59 : loss = 2.271229Eopch:     6 mean_loss = 2.593384\n",
      " 59 / 59 : loss = 2.336778Eopch:     7 mean_loss = 2.402031\n",
      " 59 / 59 : loss = 1.881093Eopch:     8 mean_loss = 2.049493\n",
      " 59 / 59 : loss = 1.681201Eopch:     9 mean_loss = 1.881113\n",
      " 59 / 59 : loss = 1.574327Eopch:    10 mean_loss = 1.746276\n",
      "best_loss = 1.746276\n",
      " 6 / 7 9 Completed buliding index in 22 seconds\n",
      "mAP=0.6449, mIoU=0.7320\n"
     ]
    }
   ],
   "source": [
    "##NNET-N1: sum of loss backward\n",
    "#Generate image pairs for model\n",
    "def onlineGenImgPairs( ):\n",
    "    idx_sf = []\n",
    "    idx_0 = np.where( np.array(trY) == 0 ) #class 0\n",
    "    idx_0 = list(idx_0[0])\n",
    "    idx_sf.extend(idx_0)\n",
    "    idx_1 = np.where( np.array(trY) == 1 ) #class 1\n",
    "    idx_1 = list(idx_1[0])\n",
    "    idx_sf.extend(idx_1)\n",
    "    random.shuffle(idx_sf)   \n",
    "    trQ_sf, trP_sf, trN_sf = [], [], []\n",
    "    trQ_y, trP_y, trN_y = [], [], []\n",
    "    trQ_m, trP_m, trN_m = [], [], []\n",
    "    for iQ in idx_sf:\n",
    "        trQ_sf.append(trI[iQ])\n",
    "        trQ_y.append(trY[iQ])\n",
    "        trQ_m.append(trM[iQ])\n",
    "        if trY[iQ] == 0:\n",
    "            idx_tmp = idx_0.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            trP_y.append(trY[iP[0]])\n",
    "            trP_m.append(trM[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_0))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "            trN_y.append(trY[iN[0]])\n",
    "            trN_m.append(trM[iN[0]])\n",
    "        elif trY[iQ] == 1:\n",
    "            idx_tmp = idx_1.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            trP_y.append(trY[iP[0]])\n",
    "            trP_m.append(trM[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_1))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "            trN_y.append(trY[iN[0]])\n",
    "            trN_m.append(trM[iN[0]])\n",
    "        else: pass\n",
    "        sys.stdout.write('\\r{} / {} '.format(len(trQ_sf),len(idx_sf)))\n",
    "        sys.stdout.flush()\n",
    "    return np.array(trQ_sf),np.array(trP_sf),np.array(trN_sf), np.array(trQ_y), np.array(trP_y), np.array(trN_y), np.array(trQ_m), np.array(trP_m), np.array(trN_m)\n",
    "#sample  triplet labels\n",
    "trQ_sf, trP_sf, trN_sf, trQ_y, trP_y, trN_y, trQ_m, trP_m, trN_m = onlineGenImgPairs() \n",
    "assert (trQ_sf.shape==trP_sf.shape and trQ_sf.shape==trN_sf.shape)\n",
    "assert (trQ_y.shape==trP_y.shape and trQ_y.shape==trN_y.shape)\n",
    "assert (trQ_m.shape==trP_m.shape and trQ_m.shape==trN_m.shape)\n",
    "assert (np.mean(np.where((np.array(trQ_y)-np.array(trP_y))!=0,1,0))==0.0)\n",
    "assert (np.mean(np.where((np.array(trQ_y)-np.array(trN_y))!=0,1,0))==1.0)\n",
    "\n",
    "#define model\n",
    "model = NNet(block=Bottleneck).cuda()#initialize model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "tl_loss  = TripletLoss(margin=0.5).cuda() #define TripletLoss \n",
    "ce_loss  = nn.CrossEntropyLoss().cuda() #define ce mutli-classes #F.log_softmax+F.nll_loss\n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "for epoch in range(10):#iteration\n",
    "    losses = []\n",
    "    shuffled_idx = np.random.permutation(np.arange(len(trQ_sf)))\n",
    "    train_q = trQ_sf[shuffled_idx]\n",
    "    train_q_y = trQ_y[shuffled_idx]\n",
    "    train_q_m = trQ_m[shuffled_idx]\n",
    "    train_p = trP_sf[shuffled_idx]\n",
    "    train_p_y = trP_y[shuffled_idx]\n",
    "    train_p_m = trP_m[shuffled_idx]\n",
    "    train_n = trN_sf[shuffled_idx]\n",
    "    train_n_y = trN_y[shuffled_idx]\n",
    "    train_n_m = trN_m[shuffled_idx]\n",
    "    num_batches = len(trQ_sf) // batchSize + 1\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trQ_sf), (i+1)*batchSize])\n",
    "        Q_batch = torch.from_numpy(train_q[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        Q_y_batch = torch.from_numpy(train_q_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        Q_m_batch = torch.from_numpy(train_q_m[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        P_batch = torch.from_numpy(train_p[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        P_y_batch = torch.from_numpy(train_p_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        P_m_batch = torch.from_numpy(train_p_m[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        N_batch = torch.from_numpy(train_n[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        N_y_batch = torch.from_numpy(train_n_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        N_m_batch = torch.from_numpy(train_n_m[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        #forword\n",
    "        Q_hash, Q_cls, Q_mask = model(Q_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        P_hash, P_cls, P_mask = model(P_batch.permute(0, 3, 1, 2))\n",
    "        N_hash, N_cls, N_mask = model(N_batch.permute(0, 3, 1, 2))\n",
    "        #loss\n",
    "        #hash_loss = tl_loss(Q_hash,P_hash,N_hash)\n",
    "        cls_loss = ce_loss(Q_cls,Q_y_batch) + ce_loss(P_cls,P_y_batch) + ce_loss(N_cls,N_y_batch) \n",
    "        mask_loss = ce_loss(Q_mask,Q_m_batch) + ce_loss(P_mask,P_m_batch) + ce_loss(N_mask,N_m_batch)\n",
    "        loss = cls_loss + mask_loss\n",
    "        loss.backward()#backward\n",
    "        optimizer.step()#update parameters\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "model = model.cpu()#release gpu memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#hash code of train data \n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize +1\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    x_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    x_hash, _, _ = best_net(x_batch.permute(0, 3, 1, 2))#forword\n",
    "    trF.extend(x_hash.cpu().data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data\n",
    "teF = [] \n",
    "num_batches = len(teY) // batchSize  +1\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teY), (i+1)*batchSize])\n",
    "    x_batch = torch.from_numpy(np.array(teI[min_idx:max_idx])).type(torch.FloatTensor).cuda()\n",
    "    x_hash, _, _ = best_net(x_batch.permute(0, 3, 1, 2))#forword\n",
    "    teF.extend(x_hash.cpu().data.numpy().tolist()) #record feature\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#evaluate\n",
    "#compute the size of lesion\n",
    "def Func_IOU_size(pred,target,n_classes = 3 ):\n",
    "    ious = []\n",
    "    # ignore IOU for background class\n",
    "    for cls in range(1,n_classes):\n",
    "        pred_inds = pred == cls\n",
    "        pred_sum = pred_inds.sum()\n",
    "        target_inds = target == cls\n",
    "        target_sum = target_inds.sum()\n",
    "        ious.append(round(float(min(pred_sum,target_sum)/max(pred_sum,target_sum)),4))\n",
    "    return np.mean(ious)\n",
    "# buliding index of trainset\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(64) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(trF, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "for topk in [10]:\n",
    "    mAP = [] #mean average precision\n",
    "    mIoU = []\n",
    "    scores, neighbors = gpu_index.search(np.array(teF).astype('float32'), k=topk)\n",
    "    for i, teVal in enumerate(teF):\n",
    "        stype = teY[i]\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        #for j in ranklist:\n",
    "        for j in neighbors[i].tolist():\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                pos_len = pos_len +1\n",
    "                mAP.append(pos_len/rank_len) \n",
    "            else: \n",
    "                mAP.append(0)\n",
    "            mIoU.append(Func_IOU_size(teM[i],trM[j]))\n",
    "    print(\"mAP={:.4f}, mIoU={:.4f}\".format(np.mean(mAP),np.mean(mIoU)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 59 / 59 : loss = 1.825545Eopch:     1 mean_loss = 1.987875\n",
      " 59 / 59 : loss = 1.014085Eopch:     2 mean_loss = 1.660885\n",
      " 59 / 59 : loss = 2.002833Eopch:     3 mean_loss = 1.380043\n",
      " 59 / 59 : loss = 1.190692Eopch:     4 mean_loss = 1.116381\n",
      " 59 / 59 : loss = 0.261126Eopch:     5 mean_loss = 0.946376\n",
      " 59 / 59 : loss = 1.889031Eopch:     6 mean_loss = 0.833036\n",
      " 59 / 59 : loss = 0.927775Eopch:     7 mean_loss = 0.531562\n",
      " 59 / 59 : loss = 0.279561Eopch:     8 mean_loss = 0.518897\n",
      " 59 / 59 : loss = 0.522377Eopch:     9 mean_loss = 0.492883\n",
      " 59 / 59 : loss = 0.084837Eopch:    10 mean_loss = 0.462096\n",
      "best_loss = 0.462096\n",
      " 6 / 7 9 Completed buliding index in 1 seconds\n",
      "mAP=0.7244, mIoU=0.7207\n"
     ]
    }
   ],
   "source": [
    "#NNET-N0 https://github.com/kuangliu/pytorch-fpn\n",
    "#Generate image pairs for model\n",
    "def onlineGenImgPairs( ):\n",
    "    idx_sf = []\n",
    "    idx_0 = np.where( np.array(trY) == 0 ) #class 0\n",
    "    idx_0 = list(idx_0[0])\n",
    "    idx_sf.extend(idx_0)\n",
    "    idx_1 = np.where( np.array(trY) == 1 ) #class 1\n",
    "    idx_1 = list(idx_1[0])\n",
    "    idx_sf.extend(idx_1)\n",
    "    random.shuffle(idx_sf)   \n",
    "    trQ_sf, trP_sf, trN_sf = [], [], []\n",
    "    trQ_y, trP_y, trN_y = [], [], []\n",
    "    trQ_m, trP_m, trN_m = [], [], []\n",
    "    for iQ in idx_sf:\n",
    "        trQ_sf.append(trI[iQ])\n",
    "        trQ_y.append(trY[iQ])\n",
    "        trQ_m.append(trM[iQ])\n",
    "        if trY[iQ] == 0:\n",
    "            idx_tmp = idx_0.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            trP_y.append(trY[iP[0]])\n",
    "            trP_m.append(trM[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_0))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "            trN_y.append(trY[iN[0]])\n",
    "            trN_m.append(trM[iN[0]])\n",
    "        elif trY[iQ] == 1:\n",
    "            idx_tmp = idx_1.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            trP_y.append(trY[iP[0]])\n",
    "            trP_m.append(trM[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_1))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "            trN_y.append(trY[iN[0]])\n",
    "            trN_m.append(trM[iN[0]])\n",
    "        else: pass\n",
    "        sys.stdout.write('\\r{} / {} '.format(len(trQ_sf),len(idx_sf)))\n",
    "        sys.stdout.flush()\n",
    "    return np.array(trQ_sf),np.array(trP_sf),np.array(trN_sf), np.array(trQ_y), np.array(trP_y), np.array(trN_y), np.array(trQ_m), np.array(trP_m), np.array(trN_m)\n",
    "#sample  triplet labels\n",
    "trQ_sf, trP_sf, trN_sf, trQ_y, trP_y, trN_y, trQ_m, trP_m, trN_m = onlineGenImgPairs() \n",
    "assert (trQ_sf.shape==trP_sf.shape and trQ_sf.shape==trN_sf.shape)\n",
    "assert (trQ_y.shape==trP_y.shape and trQ_y.shape==trN_y.shape)\n",
    "assert (trQ_m.shape==trP_m.shape and trQ_m.shape==trN_m.shape)\n",
    "assert (np.mean(np.where((np.array(trQ_y)-np.array(trP_y))!=0,1,0))==0.0)\n",
    "assert (np.mean(np.where((np.array(trQ_y)-np.array(trN_y))!=0,1,0))==1.0)\n",
    "\n",
    "#define model\n",
    "model = NNet(block=Bottleneck).cuda()#initialize model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "tl_loss  = TripletLoss(margin=0.5).cuda() #define TripletLoss \n",
    "ce_loss  = nn.CrossEntropyLoss().cuda() #define ce mutli-classes #F.log_softmax+F.nll_loss\n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "for epoch in range(10):#iteration\n",
    "    losses = []\n",
    "    shuffled_idx = np.random.permutation(np.arange(len(trQ_sf)))\n",
    "    train_q = trQ_sf[shuffled_idx]\n",
    "    train_q_y = trQ_y[shuffled_idx]\n",
    "    train_q_m = trQ_m[shuffled_idx]\n",
    "    train_p = trP_sf[shuffled_idx]\n",
    "    train_p_y = trP_y[shuffled_idx]\n",
    "    train_p_m = trP_m[shuffled_idx]\n",
    "    train_n = trN_sf[shuffled_idx]\n",
    "    train_n_y = trN_y[shuffled_idx]\n",
    "    train_n_m = trN_m[shuffled_idx]\n",
    "    num_batches = len(trQ_sf) // batchSize + 1\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trQ_sf), (i+1)*batchSize])\n",
    "        Q_batch = torch.from_numpy(train_q[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        Q_y_batch = torch.from_numpy(train_q_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        Q_m_batch = torch.from_numpy(train_q_m[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        P_batch = torch.from_numpy(train_p[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        P_y_batch = torch.from_numpy(train_p_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        P_m_batch = torch.from_numpy(train_p_m[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        N_batch = torch.from_numpy(train_n[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        N_y_batch = torch.from_numpy(train_n_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        N_m_batch = torch.from_numpy(train_n_m[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        #forword\n",
    "        Q_hash, Q_cls, Q_mask = model(Q_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        P_hash, P_cls, P_mask = model(P_batch.permute(0, 3, 1, 2))\n",
    "        N_hash, N_cls, N_mask = model(N_batch.permute(0, 3, 1, 2))\n",
    "        #loss\n",
    "        #hash_loss = tl_loss(Q_hash,P_hash,N_hash)\n",
    "        cls_loss = ce_loss(Q_cls,Q_y_batch) + ce_loss(P_cls,P_y_batch) + ce_loss(N_cls,N_y_batch) \n",
    "        #mask_loss = ce_loss(Q_mask,Q_m_batch) + ce_loss(P_mask,P_m_batch) + ce_loss(N_mask,N_m_batch)\n",
    "        loss = cls_loss #+ mask_loss\n",
    "        loss.backward()#backward\n",
    "        optimizer.step()#update parameters\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "model = model.cpu()#release gpu memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#hash code of train data \n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize +1\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    x_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    x_hash, _, _ = best_net(x_batch.permute(0, 3, 1, 2))#forword\n",
    "    trF.extend(x_hash.cpu().data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data\n",
    "teF = [] \n",
    "num_batches = len(teY) // batchSize  +1\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teY), (i+1)*batchSize])\n",
    "    x_batch = torch.from_numpy(np.array(teI[min_idx:max_idx])).type(torch.FloatTensor).cuda()\n",
    "    x_hash, _, _ = best_net(x_batch.permute(0, 3, 1, 2))#forword\n",
    "    teF.extend(x_hash.cpu().data.numpy().tolist()) #record feature\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#evaluate\n",
    "#compute the size of lesion\n",
    "def Func_IOU_size(pred,target,n_classes = 3 ):\n",
    "    ious = []\n",
    "    # ignore IOU for background class\n",
    "    for cls in range(1,n_classes):\n",
    "        pred_inds = pred == cls\n",
    "        pred_sum = pred_inds.sum()\n",
    "        target_inds = target == cls\n",
    "        target_sum = target_inds.sum()\n",
    "        ious.append(round(float(min(pred_sum,target_sum)/max(pred_sum,target_sum)),4))\n",
    "    return np.mean(ious)\n",
    "# buliding index of trainset\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(64) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(trF, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "for topk in [10]:\n",
    "    mAP = [] #mean average precision\n",
    "    mIoU = []\n",
    "    scores, neighbors = gpu_index.search(np.array(teF).astype('float32'), k=topk)\n",
    "    for i, teVal in enumerate(teF):\n",
    "        stype = teY[i]\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        #for j in ranklist:\n",
    "        for j in neighbors[i].tolist():\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                pos_len = pos_len +1\n",
    "                mAP.append(pos_len/rank_len) \n",
    "            else: \n",
    "                mAP.append(0)\n",
    "            mIoU.append(Func_IOU_size(teM[i],trM[j]))\n",
    "    print(\"mAP={:.4f}, mIoU={:.4f}\".format(np.mean(mAP),np.mean(mIoU)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
