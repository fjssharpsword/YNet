{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading faiss with AVX2 support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import math\n",
    "import random\n",
    "import heapq \n",
    "import time\n",
    "import copy\n",
    "import itertools  \n",
    "from typing import Dict, List\n",
    "from PIL import Image\n",
    "from io import StringIO,BytesIO \n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.signal import butter, lfilter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize,normalize\n",
    "from sklearn.metrics import confusion_matrix,roc_curve,accuracy_score,auc,roc_auc_score \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.ndimage import zoom\n",
    "from functools import reduce\n",
    "from scipy.io import loadmat\n",
    "from skimage.measure import block_reduce\n",
    "from collections import Counter\n",
    "from scipy.sparse import coo_matrix,hstack, vstack\n",
    "import cv2\n",
    "import faiss \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.ops as ops\n",
    "torch.cuda.set_device(6)\n",
    "print (torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3279 / 3279 The length of trainset is 3279\n",
      "365 / 365 The length of testset is 365\n",
      "Completed data handle in 102 seconds\n"
     ]
    }
   ],
   "source": [
    "tstart = time.time()\n",
    "root_dir = '/data/fjsdata/MCBIR-Ins/TNSCUI2020_train/' #the path of images\n",
    "trData = pd.read_csv(root_dir+\"trainset.csv\" , sep=',')\n",
    "teData = pd.read_csv(root_dir+\"testset.csv\" , sep=',')\n",
    "#trainset \n",
    "trN, trI, trM, trY = [],[],[],[]\n",
    "for iname, itype in np.array(trData).tolist():\n",
    "    try:\n",
    "        trN.append(iname)\n",
    "        trY.append(itype) #0 refer to Benign, and 1 refers to malignant\n",
    "        image_path = os.path.join(root_dir, 'image', iname)\n",
    "        img = cv2.resize(cv2.imread(image_path).astype(np.float32), (256, 256))#(256,256,3)\n",
    "        trI.append(img)\n",
    "        mask_path = os.path.join(root_dir,'mask', iname)\n",
    "        mask = cv2.resize(cv2.imread(mask_path).astype(np.float32), (256, 256))#(256,256)\n",
    "        trM.append(mask/255)\n",
    "    except:\n",
    "        print(iname+\":\"+str(image_path))\n",
    "    sys.stdout.write('\\r{} / {} '.format(len(trN),trData.shape[0]))\n",
    "    sys.stdout.flush()\n",
    "print('The length of trainset is %d'%len(trN))\n",
    "#testset\n",
    "teN, teI, teM, teY = [],[],[],[]\n",
    "for iname, itype in np.array(teData).tolist():\n",
    "    try:\n",
    "        teN.append(iname)\n",
    "        teY.append(itype) #0 refer to Benign, and 1 refers to malignant\n",
    "        image_path = os.path.join(root_dir, 'image', iname)\n",
    "        img = cv2.resize(cv2.imread(image_path).astype(np.float32), (256, 256))#(256,256,3)\n",
    "        teI.append(img)\n",
    "        mask_path = os.path.join(root_dir,'mask', iname)\n",
    "        mask = cv2.resize(cv2.imread(mask_path).astype(np.float32), (256, 256))#(256,256)\n",
    "        teM.append(mask/255)\n",
    "    except:\n",
    "        print(iname+\":\"+str(image_path))\n",
    "    sys.stdout.write('\\r{} / {} '.format(len(teN),teData.shape[0]))\n",
    "    sys.stdout.flush()\n",
    "print('The length of testset is %d'%len(teN))\n",
    "\n",
    "elapsed = time.time() - tstart\n",
    "print('Completed data handle in %d seconds' % int(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmac_region_coordinates(H, W, L):\n",
    "    # Almost verbatim from Tolias et al Matlab implementation.\n",
    "    # Could be heavily pythonized, but really not worth it...\n",
    "    # Desired overlap of neighboring regions\n",
    "    ovr = 0.4\n",
    "    # Possible regions for the long dimension\n",
    "    steps = np.array((2, 3, 4, 5, 6, 7), dtype=np.float32)\n",
    "    w = np.minimum(H, W)\n",
    "    \n",
    "    b = (np.maximum(H, W) - w) / (steps - 1)\n",
    "    # steps(idx) regions for long dimension. The +1 comes from Matlab\n",
    "    # 1-indexing...\n",
    "    idx = np.argmin(np.abs(((w**2 - w * b) / w**2) - ovr)) + 1\n",
    "    \n",
    "    # Region overplus per dimension\n",
    "    Wd = 0\n",
    "    Hd = 0\n",
    "    if H < W:\n",
    "        Wd = idx\n",
    "    elif H > W:\n",
    "        Hd = idx\n",
    "    \n",
    "    regions_xywh = []\n",
    "    for l in range(1, L+1):\n",
    "        wl = np.floor(2 * w / (l + 1))\n",
    "        wl2 = np.floor(wl / 2 - 1)\n",
    "        # Center coordinates\n",
    "        if l + Wd - 1 > 0:\n",
    "            b = (W - wl) / (l + Wd - 1)\n",
    "        else:\n",
    "            b = 0\n",
    "        cenW = np.floor(wl2 + b * np.arange(l - 1 + Wd + 1)) - wl2\n",
    "        # Center coordinates\n",
    "        if l + Hd - 1 > 0:\n",
    "            b = (H - wl) / (l + Hd - 1)\n",
    "        else:\n",
    "            b = 0\n",
    "        cenH = np.floor(wl2 + b * np.arange(l - 1 + Hd + 1)) - wl2\n",
    "    \n",
    "        for i_ in cenH:\n",
    "            for j_ in cenW:\n",
    "                regions_xywh.append([j_, i_, wl, wl])\n",
    "    \n",
    "    # Round the regions. Careful with the borders!\n",
    "    for i in range(len(regions_xywh)):\n",
    "        for j in range(4):\n",
    "            regions_xywh[i][j] = int(round(regions_xywh[i][j]))\n",
    "        if regions_xywh[i][0] + regions_xywh[i][2] > W:\n",
    "            regions_xywh[i][0] -= ((regions_xywh[i][0] + regions_xywh[i][2]) - W)\n",
    "        if regions_xywh[i][1] + regions_xywh[i][3] > H:\n",
    "            regions_xywh[i][1] -= ((regions_xywh[i][1] + regions_xywh[i][3]) - H)\n",
    "    return np.array(regions_xywh).astype(np.float32)\n",
    "\n",
    "def pack_regions_for_network(all_regions):\n",
    "    n_regs = np.sum([len(e) for e in all_regions])\n",
    "    R = np.zeros((n_regs, 5), dtype=np.float32)\n",
    "    cnt = 0\n",
    "    # There should be a check of overflow...\n",
    "    for i, r in enumerate(all_regions):\n",
    "        try:\n",
    "            R[cnt:cnt + r.shape[0], 0] = i\n",
    "            R[cnt:cnt + r.shape[0], 1:] = r\n",
    "            cnt += r.shape[0]\n",
    "        except:\n",
    "            continue\n",
    "    assert cnt == n_regs\n",
    "    R = R[:n_regs]\n",
    "    # regs where in xywh format. R is in xyxy format, where the last coordinate is included. Therefore...\n",
    "    R[:n_regs, 3] = R[:n_regs, 1] + R[:n_regs, 3] - 1\n",
    "    R[:n_regs, 4] = R[:n_regs, 2] + R[:n_regs, 4] - 1\n",
    "    return R\n",
    "\n",
    "class L2Normalization(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we construct three nn.Linear instances that we will use\n",
    "        in the forward pass.\n",
    "        \"\"\"\n",
    "        super(L2Normalization, self).__init__()\n",
    "        self.eps = 1e-8\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if x.is_cuda:\n",
    "            caped_eps = Variable(torch.Tensor([self.eps])).cuda(torch.cuda.device_of(x).idx)\n",
    "        else:\n",
    "            caped_eps = Variable(torch.Tensor([self.eps]))\n",
    "        x = torch.div(x.transpose(0,1),x.max(1)[0]).transpose(0,1) # max_normed\n",
    "        norm = torch.norm(x,2,1) + caped_eps.expand(x.size()[0])\n",
    "        y = torch.div(x.transpose(0,1),norm).transpose(0,1)\n",
    "        return y\n",
    "    \n",
    "class RoIPool(nn.Module):\n",
    "    def __init__(self, pooled_height, pooled_width, spatial_scale):\n",
    "        super(RoIPool, self).__init__()\n",
    "        self.pooled_width = int(pooled_width)\n",
    "        self.pooled_height = int(pooled_height)\n",
    "        self.spatial_scale = float(spatial_scale)\n",
    "\n",
    "    def forward(self, features, rois):\n",
    "        batch_size, num_channels, data_height, data_width = features.size()\n",
    "        num_rois = rois.size()[0]\n",
    "        \n",
    "        outputs = Variable(torch.zeros(num_rois, num_channels,\n",
    "                                       self.pooled_height,\n",
    "                                       self.pooled_width))\n",
    "        if features.is_cuda:\n",
    "            outputs = outputs.cuda(torch.cuda.device_of(features).idx)\n",
    "\n",
    "        for roi_ind, roi in enumerate(rois):\n",
    "            batch_ind = int(roi[0].item())\n",
    "            roi_start_w, roi_start_h, roi_end_w, roi_end_h = torch.round(roi[1:]* self.spatial_scale).data.cpu().numpy().astype(int)\n",
    "            roi_width = max(roi_end_w - roi_start_w + 1, 1)\n",
    "            roi_height = max(roi_end_h - roi_start_h + 1, 1)\n",
    "            bin_size_w = float(roi_width) / float(self.pooled_width)\n",
    "            bin_size_h = float(roi_height) / float(self.pooled_height)\n",
    "\n",
    "            for ph in range(self.pooled_height):\n",
    "                hstart = int(np.floor(ph * bin_size_h))\n",
    "                hend = int(np.ceil((ph + 1) * bin_size_h))\n",
    "                hstart = min(data_height, max(0, hstart + roi_start_h))\n",
    "                hend = min(data_height, max(0, hend + roi_start_h))\n",
    "                for pw in range(self.pooled_width):\n",
    "                    wstart = int(np.floor(pw * bin_size_w))\n",
    "                    wend = int(np.ceil((pw + 1) * bin_size_w))\n",
    "                    wstart = min(data_width, max(0, wstart + roi_start_w))\n",
    "                    wend = min(data_width, max(0, wend + roi_start_w))\n",
    "\n",
    "                    is_empty = (hend <= hstart) or(wend <= wstart)\n",
    "                    if is_empty:\n",
    "                        outputs[roi_ind, :, ph, pw] = 0\n",
    "                    else:\n",
    "                        data = features[batch_ind]\n",
    "                        outputs[roi_ind, :, ph, pw] = torch.max(\n",
    "                            torch.max(data[:, hstart:hend, wstart:wend], 1, keepdim = True)[0], 2, keepdim = True)[0].view(-1)  # noqa\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "class ContextAwareRegionalAttentionNetwork(nn.Module):\n",
    "    def __init__(self, spatial_scale, pooled_height = 1, pooled_width = 1):\n",
    "        super(ContextAwareRegionalAttentionNetwork, self).__init__()\n",
    "        self.pooled_width = int(pooled_width)\n",
    "        self.pooled_height = int(pooled_height)\n",
    "        self.spatial_scale = float(spatial_scale)\n",
    "        \n",
    "        self.conv_att_1 = nn.Conv1d(4096, 64, 1, padding=0)\n",
    "        self.sp_att_1 = nn.Softplus()\n",
    "        self.conv_att_2 = nn.Conv1d(64, 1, 1, padding=0)\n",
    "        self.sp_att_2 = nn.Softplus()\n",
    "        \n",
    "\n",
    "    def forward(self, features, rois):\n",
    "        batch_size, num_channels, data_height, data_width = features.size()\n",
    "        num_rois = rois.size()[0]\n",
    "        \n",
    "        outputs = Variable(torch.zeros(num_rois, num_channels*2,\n",
    "                                       self.pooled_height,\n",
    "                                       self.pooled_width))\n",
    "        if features.is_cuda:\n",
    "            outputs = outputs.cuda(torch.cuda.device_of(features).idx)\n",
    "            \n",
    "        # Based on roi pooling code of pytorch but, the only difference is to change max pooling to mean pooling\n",
    "        for roi_ind, roi in enumerate(rois):\n",
    "            batch_ind = int(roi[0].item())\n",
    "            roi_start_w, roi_start_h, roi_end_w, roi_end_h =  torch.round(roi[1:]* self.spatial_scale).data.cpu().numpy().astype(int)\n",
    "            roi_width = max(roi_end_w - roi_start_w + 1, 1)\n",
    "            roi_height = max(roi_end_h - roi_start_h + 1, 1)\n",
    "            bin_size_w = float(roi_width) / float(self.pooled_width)\n",
    "            bin_size_h = float(roi_height) / float(self.pooled_height)\n",
    "\n",
    "            for ph in range(self.pooled_height):\n",
    "                hstart = int(np.floor(ph * bin_size_h))\n",
    "                hend = int(np.ceil((ph + 1) * bin_size_h))\n",
    "                hstart = min(data_height, max(0, hstart + roi_start_h))\n",
    "                hend = min(data_height, max(0, hend + roi_start_h))\n",
    "                for pw in range(self.pooled_width):\n",
    "                    wstart = int(np.floor(pw * bin_size_w))\n",
    "                    wend = int(np.ceil((pw + 1) * bin_size_w))\n",
    "                    wstart = min(data_width, max(0, wstart + roi_start_w))\n",
    "                    wend = min(data_width, max(0, wend + roi_start_w))\n",
    "\n",
    "                    is_empty = (hend <= hstart) or(wend <= wstart)\n",
    "                    if is_empty:\n",
    "                        outputs[roi_ind, :, ph, pw] = 0\n",
    "                    else:\n",
    "                        data = features[batch_ind]\n",
    "                        # mean pooling with both of regional feature map and global feature map\n",
    "                        outputs[roi_ind, :, ph, pw] = torch.cat((torch.mean(\n",
    "                            torch.mean(data[:, hstart:hend, wstart:wend], 1, keepdim = True), 2, keepdim = True).view(-1)\n",
    "                            ,torch.mean(\n",
    "                            torch.mean(data, 1, keepdim = True), 2, keepdim = True).view(-1)), 0 )  # noqa\n",
    "                        \n",
    "        # Reshpae\n",
    "        outputs = outputs.squeeze(2).squeeze(2)\n",
    "        outputs = outputs.transpose(0,1).unsqueeze(0) # (1, # channel, #batch * # regions)\n",
    "        #Calculate regional attention weights with context-aware regional feature vectors\n",
    "        k = self.sp_att_1(self.conv_att_1(outputs))\n",
    "        k = self.sp_att_2(self.conv_att_2(k)) # (1, 1, #batch * # regions)\n",
    "        k = torch.squeeze(k,1)\n",
    "        \n",
    "        return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/luyajie/triplet-deep-hash-pytorch#triplet-deep-hash-pytorch            \n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=0.5):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin #margin threshold\n",
    "        self.mse_loss = nn.MSELoss(reduction='none')\n",
    "    \n",
    "    def forward(self,H_q,H_p,H_n):    \n",
    "        margin_val = self.margin * H_q.shape[1]\n",
    "        squared_loss_pos = torch.mean(self.mse_loss(H_q, H_p), dim=1)\n",
    "        squared_loss_neg = torch.mean(self.mse_loss(H_q, H_n), dim=1)\n",
    "        zeros = torch.zeros_like(squared_loss_neg)\n",
    "        loss  = torch.max(zeros, margin_val - squared_loss_neg + squared_loss_pos)\n",
    "        return torch.mean(loss)\n",
    "    \n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "    \n",
    "class NNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks=[2,2,2,2], n_classes=2, code_size=64):\n",
    "        super(NNet, self).__init__()\n",
    "        # Bottom-up layers，classifcation loss\n",
    "        self.in_planes = 8  #3 D->64 channels\n",
    "        self.conv1 = nn.Conv2d(3, 8, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "\n",
    "        self.layer2 = self._make_layer(block, 8, num_blocks[0], stride=1)\n",
    "        self.layer3 = self._make_layer(block, 16, num_blocks[1], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 32, num_blocks[2], stride=2)\n",
    "        self.layer5 = self._make_layer(block, 64, num_blocks[3], stride=2)\n",
    "        self.conv6 = nn.Conv2d(256, 32, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(32*4*4,64)#code_size:length of hash code\n",
    "        self.fc2 = nn.Linear(code_size,n_classes) #num_classes:number of classes\n",
    "\n",
    "        # Top-down layer, segmentation loss\n",
    "        self.toplayer = nn.Conv2d(256, 32, kernel_size=1, stride=1, padding=0)  # Reduce channels\n",
    "        \n",
    "        self.latlayer1 = nn.Conv2d(128, 32, kernel_size=1, stride=1, padding=0)# Lateral layers\n",
    "        self.latlayer2 = nn.Conv2d(64, 32, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        self.upsample = nn.Upsample((256,256), mode='bilinear',align_corners=True)\n",
    "        self.conv7 = nn.Conv2d(32, 3, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(3)#mask \n",
    "        \n",
    "        # Hash layer, ranking loss\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _upsample_add(self, x, y):\n",
    "        '''Upsample and add two feature maps.\n",
    "        Args:\n",
    "          x: (Variable) top feature map to be upsampled.\n",
    "          y: (Variable) lateral feature map.\n",
    "        Returns:\n",
    "          (Variable) added feature map.\n",
    "        Note in PyTorch, when input size is odd, the upsampled feature map\n",
    "        with `F.upsample(..., scale_factor=2, mode='nearest')`\n",
    "        maybe not equal to the lateral feature map size.\n",
    "        e.g.\n",
    "        original input size: [N,_,15,15] ->\n",
    "        conv2d feature map size: [N,_,8,8] ->\n",
    "        upsampled feature map size: [N,_,16,16]\n",
    "        So we choose bilinear upsample which supports arbitrary output sizes.\n",
    "        '''\n",
    "        _,_,H,W = y.size()\n",
    "        return F.upsample(x, size=(H,W), mode='bilinear') + y\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Bottom-up, classifcation loss\n",
    "        c1 = F.relu(self.bn1(self.conv1(x)))#(3,256,256)->(8,128,128)\n",
    "        c1 = F.max_pool2d(c1, kernel_size=3, stride=2, padding=1)#(8,128,128)->(8,64,64)\n",
    "        \n",
    "        c2 = self.layer2(c1)#(8,64,64)->(32,64,64)\n",
    "        c3 = self.layer3(c2)#(32,64,64)->(64,32,32)\n",
    "        c4 = self.layer4(c3)#(64,32,32)->(128,16,16)\n",
    "        c5 = self.layer5(c4)#(128,16,16)->(256,8,8)\n",
    "        \n",
    "        c6 = self.conv6(c5)#(256,8,8)->(32,4,4) \n",
    "        c6 = c6.view(c6.size(0), -1)#conv->linear\n",
    "        c_hash = self.fc1(c6)\n",
    "        c_cls  = self.fc2(c_hash)\n",
    "        \n",
    "        # Top-down, segmentation loss\n",
    "        s5 = self.toplayer(c5)#(256,8,8)->(32,8,8)\n",
    "        s4 = self._upsample_add(s5, self.latlayer1(c4))#{(32,8,8),(32, 16, 16)}->(32, 16, 16)\n",
    "        s3 = self._upsample_add(s4, self.latlayer2(c3))#{(32, 16, 16),(32, 32, 32)}->(32, 32, 32)\n",
    "        s2 = self._upsample_add(s3, c2) #{(32, 32, 32),(32, 64, 64)}->(32, 64, 64)\n",
    "        \n",
    "        s1 = self.upsample(s2)#(32, 64, 64)->(32, 256, 256)\n",
    "        s_mask = self.bn2(self.conv7(s1))#(32, 256, 256)->(3, 256, 256)\n",
    "        \n",
    "        #Hash, ranking loss\n",
    "        \n",
    "\n",
    "        return c_hash,c_cls,s_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate image pairs for model\n",
    "def onlineGenImgPairs( ):\n",
    "    idx_sf = []\n",
    "    idx_0 = np.where( np.array(trY) == 0 ) #class 0\n",
    "    idx_0 = list(idx_0[0])\n",
    "    idx_sf.extend(idx_0)\n",
    "    idx_1 = np.where( np.array(trY) == 1 ) #class 1\n",
    "    idx_1 = list(idx_1[0])\n",
    "    idx_sf.extend(idx_1)\n",
    "    random.shuffle(idx_sf)   \n",
    "    trQ_sf, trP_sf, trN_sf = [], [], []\n",
    "    trQ_y, trP_y, trN_y = [], [], []\n",
    "    trQ_m, trP_m, trN_m = [], [], []\n",
    "    for iQ in idx_sf:\n",
    "        trQ_sf.append(trI[iQ])\n",
    "        trQ_y.append(trY[iQ])\n",
    "        trQ_m.append(trM[iQ])\n",
    "        if trY[iQ] == 0:\n",
    "            idx_tmp = idx_0.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            trP_y.append(trY[iP[0]])\n",
    "            trP_m.append(trM[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_0))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "            trN_y.append(trY[iN[0]])\n",
    "            trN_m.append(trM[iN[0]])\n",
    "        elif trY[iQ] == 1:\n",
    "            idx_tmp = idx_1.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            trP_y.append(trY[iP[0]])\n",
    "            trP_m.append(trM[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_1))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "            trN_y.append(trY[iN[0]])\n",
    "            trN_m.append(trM[iN[0]])\n",
    "        else: pass\n",
    "        sys.stdout.write('\\r{} / {} '.format(len(trQ_sf),len(idx_sf)))\n",
    "        sys.stdout.flush()\n",
    "    return np.array(trQ_sf),np.array(trP_sf),np.array(trN_sf), np.array(trQ_y), np.array(trP_y), np.array(trN_y), np.array(trQ_m), np.array(trP_m), np.array(trN_m)\n",
    "#sample  triplet labels\n",
    "#trQ_sf, trP_sf, trN_sf, trQ_y, trP_y, trN_y, trQ_m, trP_m, trN_m = onlineGenImgPairs() \n",
    "assert (trQ_sf.shape==trP_sf.shape and trQ_sf.shape==trN_sf.shape)\n",
    "assert (trQ_y.shape==trP_y.shape and trQ_y.shape==trN_y.shape)\n",
    "assert (trQ_m.shape==trP_m.shape and trQ_m.shape==trN_m.shape)\n",
    "assert (np.mean(np.where((np.array(trQ_y)-np.array(trP_y))!=0,1,0))==0.0)\n",
    "assert (np.mean(np.where((np.array(trQ_y)-np.array(trN_y))!=0,1,0))==1.0)\n",
    "\n",
    "#define model\n",
    "model = NNet(block=Bottleneck).cuda()#initialize model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "tl_loss  = TripletLoss(margin=0.5).cuda() #define TripletLoss \n",
    "ce_loss  = nn.CrossEntropyLoss().cuda() #define ce mutli-classes #F.log_softmax+F.nll_loss\n",
    "mse_loss  = nn.MSELoss().cuda() #define mseloss\n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "for epoch in range(10):#iteration\n",
    "    losses = []\n",
    "    shuffled_idx = np.random.permutation(np.arange(len(trQ_sf)))\n",
    "    train_q = trQ_sf[shuffled_idx]\n",
    "    train_q_y = trQ_y[shuffled_idx]\n",
    "    train_q_m = trQ_m[shuffled_idx]\n",
    "    train_p = trP_sf[shuffled_idx]\n",
    "    train_p_y = trP_y[shuffled_idx]\n",
    "    train_p_m = trP_m[shuffled_idx]\n",
    "    train_n = trN_sf[shuffled_idx]\n",
    "    train_n_y = trN_y[shuffled_idx]\n",
    "    train_n_m = trN_m[shuffled_idx]\n",
    "    num_batches = len(trQ_sf) // batchSize + 1\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trQ_sf), (i+1)*batchSize])\n",
    "        Q_batch = torch.from_numpy(train_q[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        Q_y_batch = torch.from_numpy(train_q_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        Q_m_batch = torch.from_numpy(train_q_m[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        P_batch = torch.from_numpy(train_p[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        P_y_batch = torch.from_numpy(train_p_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        P_m_batch = torch.from_numpy(train_p_m[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        N_batch = torch.from_numpy(train_n[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        N_y_batch = torch.from_numpy(train_n_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        N_m_batch = torch.from_numpy(train_n_m[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        #forword\n",
    "        Q_hash, Q_cls, Q_mask = model(Q_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        P_hash, P_cls, P_mask = model(P_batch.permute(0, 3, 1, 2))\n",
    "        N_hash, N_cls, N_mask = model(N_batch.permute(0, 3, 1, 2))\n",
    "        #loss\n",
    "        cls_loss = ce_loss(Q_cls,Q_y_batch) + ce_loss(P_cls,P_y_batch) + ce_loss(N_cls,N_y_batch)\n",
    "        cls_loss.backward(retain_graph=True) #buffer\n",
    "        mask_loss = mse_loss(Q_mask,Q_m_batch.permute(0, 3, 1, 2)) + mse_loss(P_mask,P_m_batch.permute(0, 3, 1, 2)) + mse_loss(N_mask,N_m_batch.permute(0, 3, 1, 2))\n",
    "        mask_loss.backward()\n",
    "        optimizer.step()#update parameters\n",
    "        #show loss\n",
    "        loss = cls_loss + mask_loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "#model = model.cpu()#release gpu memory\n",
    "#torch.cuda.empty_cache()\n",
    "\n",
    "#hash code of train data \n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize +1\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    x_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    x_hash, _, _ = best_net(x_batch.permute(0, 3, 1, 2))#forword\n",
    "    trF.extend(x_hash.cpu().data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data\n",
    "teF = [] \n",
    "num_batches = len(teY) // batchSize  +1\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teY), (i+1)*batchSize])\n",
    "    x_batch = torch.from_numpy(np.array(teI[min_idx:max_idx])).type(torch.FloatTensor).cuda()\n",
    "    x_hash, _, _ = best_net(x_batch.permute(0, 3, 1, 2))#forword\n",
    "    teF.extend(x_hash.cpu().data.numpy().tolist()) #record feature\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#evaluate\n",
    "#compute the size of lesion\n",
    "def Func_IOU_size(pred,target):\n",
    "    ious = []\n",
    "    # ignore IOU for background class\n",
    "    pred_inds = pred != 0\n",
    "    pred_sum = pred_inds.sum()\n",
    "    target_inds = target != 0\n",
    "    target_sum = target_inds.sum()\n",
    "    ious.append(round(float(min(pred_sum,target_sum)/max(pred_sum,target_sum)),4))\n",
    "    return np.mean(ious)\n",
    "# buliding index of trainset\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(64) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(trF, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "for topk in [10]:\n",
    "    mAP = [] #mean average precision\n",
    "    mIoU = []\n",
    "    scores, neighbors = gpu_index.search(np.array(teF).astype('float32'), k=topk)\n",
    "    for i, teVal in enumerate(teF):\n",
    "        stype = teY[i]\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        #for j in ranklist:\n",
    "        for j in neighbors[i].tolist():\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                pos_len = pos_len +1\n",
    "                mAP.append(pos_len/rank_len) \n",
    "            else: \n",
    "                mAP.append(0)\n",
    "            mIoU.append(Func_IOU_size(teM[i],trM[j]))\n",
    "    print(\"mAP={:.4f}, mIoU={:.4f}\".format(np.mean(mAP),np.mean(mIoU)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#release gpu memory\n",
    "model = model.cpu()\n",
    "tl_loss=tl_loss.cpu()\n",
    "mse_loss=mse_loss.cpu()\n",
    "ce_loss = ce_loss.cpu()\n",
    "Q_batch = Q_batch.cpu()\n",
    "Q_y_batch = Q_y_batch.cpu()\n",
    "P_batch = P_batch.cpu()\n",
    "P_y_batch = P_y_batch.cpu()\n",
    "N_batch = N_batch.cpu()\n",
    "N_y_batch = N_y_batch.cpu()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "============================================Demo============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 328 / 328 : loss = 1.940378Eopch:     1 mean_loss = 2.406426\n",
      " 328 / 328 : loss = 1.760062Eopch:     2 mean_loss = 1.596185\n",
      " 328 / 328 : loss = 1.942828Eopch:     3 mean_loss = 1.074813\n",
      " 328 / 328 : loss = 0.442164Eopch:     4 mean_loss = 0.721531\n",
      " 328 / 328 : loss = 0.311932Eopch:     5 mean_loss = 0.534451\n",
      " 328 / 328 : loss = 0.424887Eopch:     6 mean_loss = 0.460156\n",
      " 328 / 328 : loss = 0.624144Eopch:     7 mean_loss = 0.384933\n",
      " 328 / 328 : loss = 0.558857Eopch:     8 mean_loss = 0.356752\n",
      " 328 / 328 : loss = 0.158232Eopch:     9 mean_loss = 0.343381\n",
      " 328 / 328 : loss = 0.185198Eopch:    10 mean_loss = 0.333583\n",
      "best_loss = 0.333583\n",
      " 36 / 37 8 Completed buliding index in 22 seconds\n",
      "mAP=0.6155, mIoU=0.4248\n"
     ]
    }
   ],
   "source": [
    "#NNET-N1:  backward separately\n",
    "#Generate image pairs for model\n",
    "def onlineGenImgPairs( ):\n",
    "    idx_sf = []\n",
    "    idx_0 = np.where( np.array(trY) == 0 ) #class 0\n",
    "    idx_0 = list(idx_0[0])\n",
    "    idx_sf.extend(idx_0)\n",
    "    idx_1 = np.where( np.array(trY) == 1 ) #class 1\n",
    "    idx_1 = list(idx_1[0])\n",
    "    idx_sf.extend(idx_1)\n",
    "    random.shuffle(idx_sf)   \n",
    "    trQ_sf, trP_sf, trN_sf = [], [], []\n",
    "    trQ_y, trP_y, trN_y = [], [], []\n",
    "    trQ_m, trP_m, trN_m = [], [], []\n",
    "    for iQ in idx_sf:\n",
    "        trQ_sf.append(trI[iQ])\n",
    "        trQ_y.append(trY[iQ])\n",
    "        trQ_m.append(trM[iQ])\n",
    "        if trY[iQ] == 0:\n",
    "            idx_tmp = idx_0.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            trP_y.append(trY[iP[0]])\n",
    "            trP_m.append(trM[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_0))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "            trN_y.append(trY[iN[0]])\n",
    "            trN_m.append(trM[iN[0]])\n",
    "        elif trY[iQ] == 1:\n",
    "            idx_tmp = idx_1.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            trP_y.append(trY[iP[0]])\n",
    "            trP_m.append(trM[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_1))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "            trN_y.append(trY[iN[0]])\n",
    "            trN_m.append(trM[iN[0]])\n",
    "        else: pass\n",
    "        sys.stdout.write('\\r{} / {} '.format(len(trQ_sf),len(idx_sf)))\n",
    "        sys.stdout.flush()\n",
    "    return np.array(trQ_sf),np.array(trP_sf),np.array(trN_sf), np.array(trQ_y), np.array(trP_y), np.array(trN_y), np.array(trQ_m), np.array(trP_m), np.array(trN_m)\n",
    "#sample  triplet labels\n",
    "#trQ_sf, trP_sf, trN_sf, trQ_y, trP_y, trN_y, trQ_m, trP_m, trN_m = onlineGenImgPairs() \n",
    "assert (trQ_sf.shape==trP_sf.shape and trQ_sf.shape==trN_sf.shape)\n",
    "assert (trQ_y.shape==trP_y.shape and trQ_y.shape==trN_y.shape)\n",
    "assert (trQ_m.shape==trP_m.shape and trQ_m.shape==trN_m.shape)\n",
    "assert (np.mean(np.where((np.array(trQ_y)-np.array(trP_y))!=0,1,0))==0.0)\n",
    "assert (np.mean(np.where((np.array(trQ_y)-np.array(trN_y))!=0,1,0))==1.0)\n",
    "\n",
    "#define model\n",
    "model = NNet(block=Bottleneck).cuda()#initialize model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "tl_loss  = TripletLoss(margin=0.5).cuda() #define TripletLoss \n",
    "ce_loss  = nn.CrossEntropyLoss().cuda() #define ce mutli-classes #F.log_softmax+F.nll_loss\n",
    "mse_loss  = nn.MSELoss().cuda() #define mseloss\n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "for epoch in range(10):#iteration\n",
    "    losses = []\n",
    "    shuffled_idx = np.random.permutation(np.arange(len(trQ_sf)))\n",
    "    train_q = trQ_sf[shuffled_idx]\n",
    "    train_q_y = trQ_y[shuffled_idx]\n",
    "    train_q_m = trQ_m[shuffled_idx]\n",
    "    train_p = trP_sf[shuffled_idx]\n",
    "    train_p_y = trP_y[shuffled_idx]\n",
    "    train_p_m = trP_m[shuffled_idx]\n",
    "    train_n = trN_sf[shuffled_idx]\n",
    "    train_n_y = trN_y[shuffled_idx]\n",
    "    train_n_m = trN_m[shuffled_idx]\n",
    "    num_batches = len(trQ_sf) // batchSize + 1\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trQ_sf), (i+1)*batchSize])\n",
    "        Q_batch = torch.from_numpy(train_q[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        Q_y_batch = torch.from_numpy(train_q_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        Q_m_batch = torch.from_numpy(train_q_m[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        P_batch = torch.from_numpy(train_p[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        P_y_batch = torch.from_numpy(train_p_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        P_m_batch = torch.from_numpy(train_p_m[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        N_batch = torch.from_numpy(train_n[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        N_y_batch = torch.from_numpy(train_n_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        N_m_batch = torch.from_numpy(train_n_m[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        #forword\n",
    "        Q_hash, Q_cls, Q_mask = model(Q_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        P_hash, P_cls, P_mask = model(P_batch.permute(0, 3, 1, 2))\n",
    "        N_hash, N_cls, N_mask = model(N_batch.permute(0, 3, 1, 2))\n",
    "        #loss\n",
    "        #hash_loss = tl_loss(Q_hash,P_hash,N_hash)\n",
    "        cls_loss = ce_loss(Q_cls,Q_y_batch) + ce_loss(P_cls,P_y_batch) + ce_loss(N_cls,N_y_batch)\n",
    "        cls_loss.backward(retain_graph=True) #buffer\n",
    "        mask_loss = mse_loss(Q_mask,Q_m_batch.permute(0, 3, 1, 2)) + mse_loss(P_mask,P_m_batch.permute(0, 3, 1, 2)) + mse_loss(N_mask,N_m_batch.permute(0, 3, 1, 2))\n",
    "        mask_loss.backward()\n",
    "        optimizer.step()#update parameters\n",
    "        #show loss\n",
    "        loss = cls_loss + mask_loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "#model = model.cpu()#release gpu memory\n",
    "#torch.cuda.empty_cache()\n",
    "\n",
    "#hash code of train data \n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize +1\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    x_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    x_hash, _, _ = best_net(x_batch.permute(0, 3, 1, 2))#forword\n",
    "    trF.extend(x_hash.cpu().data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data\n",
    "teF = [] \n",
    "num_batches = len(teY) // batchSize  +1\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teY), (i+1)*batchSize])\n",
    "    x_batch = torch.from_numpy(np.array(teI[min_idx:max_idx])).type(torch.FloatTensor).cuda()\n",
    "    x_hash, _, _ = best_net(x_batch.permute(0, 3, 1, 2))#forword\n",
    "    teF.extend(x_hash.cpu().data.numpy().tolist()) #record feature\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#evaluate\n",
    "#compute the size of lesion\n",
    "def Func_IOU_size(pred,target):\n",
    "    ious = []\n",
    "    # ignore IOU for background class\n",
    "    pred_inds = pred != 0\n",
    "    pred_sum = pred_inds.sum()\n",
    "    target_inds = target != 0\n",
    "    target_sum = target_inds.sum()\n",
    "    ious.append(round(float(min(pred_sum,target_sum)/max(pred_sum,target_sum)),4))\n",
    "    return np.mean(ious)\n",
    "# buliding index of trainset\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(64) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(trF, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "for topk in [10]:\n",
    "    mAP = [] #mean average precision\n",
    "    mIoU = []\n",
    "    scores, neighbors = gpu_index.search(np.array(teF).astype('float32'), k=topk)\n",
    "    for i, teVal in enumerate(teF):\n",
    "        stype = teY[i]\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        #for j in ranklist:\n",
    "        for j in neighbors[i].tolist():\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                pos_len = pos_len +1\n",
    "                mAP.append(pos_len/rank_len) \n",
    "            else: \n",
    "                mAP.append(0)\n",
    "            mIoU.append(Func_IOU_size(teM[i],trM[j]))\n",
    "    print(\"mAP={:.4f}, mIoU={:.4f}\".format(np.mean(mAP),np.mean(mIoU)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 328 / 328 : loss = 1.810328Eopch:     1 mean_loss = 1.996622\n",
      " 328 / 328 : loss = 0.518584Eopch:     2 mean_loss = 1.346461\n",
      " 328 / 328 : loss = 1.130359Eopch:     3 mean_loss = 0.890692\n",
      " 328 / 328 : loss = 0.452995Eopch:     4 mean_loss = 0.628943\n",
      " 328 / 328 : loss = 1.042231Eopch:     5 mean_loss = 0.477920\n",
      " 328 / 328 : loss = 0.380489Eopch:     6 mean_loss = 0.402193\n",
      " 328 / 328 : loss = 0.541506Eopch:     7 mean_loss = 0.360090\n",
      " 328 / 328 : loss = 0.449224Eopch:     8 mean_loss = 0.337316\n",
      " 328 / 328 : loss = 0.188263Eopch:     9 mean_loss = 0.297242\n",
      " 328 / 328 : loss = 0.739658Eopch:    10 mean_loss = 0.324664\n",
      "best_loss = 0.297242\n",
      " 36 / 37 8 Completed buliding index in 1 seconds\n",
      "mAP=0.5892, mIoU=0.4218\n"
     ]
    }
   ],
   "source": [
    "#NNET-N1:  sum of loss backward\n",
    "#Generate image pairs for model\n",
    "def onlineGenImgPairs( ):\n",
    "    idx_sf = []\n",
    "    idx_0 = np.where( np.array(trY) == 0 ) #class 0\n",
    "    idx_0 = list(idx_0[0])\n",
    "    idx_sf.extend(idx_0)\n",
    "    idx_1 = np.where( np.array(trY) == 1 ) #class 1\n",
    "    idx_1 = list(idx_1[0])\n",
    "    idx_sf.extend(idx_1)\n",
    "    random.shuffle(idx_sf)   \n",
    "    trQ_sf, trP_sf, trN_sf = [], [], []\n",
    "    trQ_y, trP_y, trN_y = [], [], []\n",
    "    trQ_m, trP_m, trN_m = [], [], []\n",
    "    for iQ in idx_sf:\n",
    "        trQ_sf.append(trI[iQ])\n",
    "        trQ_y.append(trY[iQ])\n",
    "        trQ_m.append(trM[iQ])\n",
    "        if trY[iQ] == 0:\n",
    "            idx_tmp = idx_0.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            trP_y.append(trY[iP[0]])\n",
    "            trP_m.append(trM[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_0))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "            trN_y.append(trY[iN[0]])\n",
    "            trN_m.append(trM[iN[0]])\n",
    "        elif trY[iQ] == 1:\n",
    "            idx_tmp = idx_1.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            trP_y.append(trY[iP[0]])\n",
    "            trP_m.append(trM[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_1))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "            trN_y.append(trY[iN[0]])\n",
    "            trN_m.append(trM[iN[0]])\n",
    "        else: pass\n",
    "        sys.stdout.write('\\r{} / {} '.format(len(trQ_sf),len(idx_sf)))\n",
    "        sys.stdout.flush()\n",
    "    return np.array(trQ_sf),np.array(trP_sf),np.array(trN_sf), np.array(trQ_y), np.array(trP_y), np.array(trN_y), np.array(trQ_m), np.array(trP_m), np.array(trN_m)\n",
    "#sample  triplet labels\n",
    "#trQ_sf, trP_sf, trN_sf, trQ_y, trP_y, trN_y, trQ_m, trP_m, trN_m = onlineGenImgPairs() \n",
    "assert (trQ_sf.shape==trP_sf.shape and trQ_sf.shape==trN_sf.shape)\n",
    "assert (trQ_y.shape==trP_y.shape and trQ_y.shape==trN_y.shape)\n",
    "assert (trQ_m.shape==trP_m.shape and trQ_m.shape==trN_m.shape)\n",
    "assert (np.mean(np.where((np.array(trQ_y)-np.array(trP_y))!=0,1,0))==0.0)\n",
    "assert (np.mean(np.where((np.array(trQ_y)-np.array(trN_y))!=0,1,0))==1.0)\n",
    "\n",
    "#define model\n",
    "model = NNet(block=Bottleneck).cuda()#initialize model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "tl_loss  = TripletLoss(margin=0.5).cuda() #define TripletLoss \n",
    "ce_loss  = nn.CrossEntropyLoss().cuda() #define ce mutli-classes #F.log_softmax+F.nll_loss\n",
    "mse_loss  = nn.MSELoss().cuda() #define mseloss\n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "for epoch in range(10):#iteration\n",
    "    losses = []\n",
    "    shuffled_idx = np.random.permutation(np.arange(len(trQ_sf)))\n",
    "    train_q = trQ_sf[shuffled_idx]\n",
    "    train_q_y = trQ_y[shuffled_idx]\n",
    "    train_q_m = trQ_m[shuffled_idx]\n",
    "    train_p = trP_sf[shuffled_idx]\n",
    "    train_p_y = trP_y[shuffled_idx]\n",
    "    train_p_m = trP_m[shuffled_idx]\n",
    "    train_n = trN_sf[shuffled_idx]\n",
    "    train_n_y = trN_y[shuffled_idx]\n",
    "    train_n_m = trN_m[shuffled_idx]\n",
    "    num_batches = len(trQ_sf) // batchSize + 1\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trQ_sf), (i+1)*batchSize])\n",
    "        Q_batch = torch.from_numpy(train_q[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        Q_y_batch = torch.from_numpy(train_q_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        Q_m_batch = torch.from_numpy(train_q_m[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        P_batch = torch.from_numpy(train_p[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        P_y_batch = torch.from_numpy(train_p_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        P_m_batch = torch.from_numpy(train_p_m[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        N_batch = torch.from_numpy(train_n[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        N_y_batch = torch.from_numpy(train_n_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        N_m_batch = torch.from_numpy(train_n_m[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        #forword\n",
    "        Q_hash, Q_cls, Q_mask = model(Q_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        P_hash, P_cls, P_mask = model(P_batch.permute(0, 3, 1, 2))\n",
    "        N_hash, N_cls, N_mask = model(N_batch.permute(0, 3, 1, 2))\n",
    "        #loss\n",
    "        #hash_loss = tl_loss(Q_hash,P_hash,N_hash)\n",
    "        cls_loss = ce_loss(Q_cls,Q_y_batch) + ce_loss(P_cls,P_y_batch) + ce_loss(N_cls,N_y_batch)\n",
    "        #cls_loss.backward(retain_graph=True) #buffer\n",
    "        mask_loss = mse_loss(Q_mask,Q_m_batch.permute(0, 3, 1, 2)) + mse_loss(P_mask,P_m_batch.permute(0, 3, 1, 2)) + mse_loss(N_mask,N_m_batch.permute(0, 3, 1, 2))\n",
    "        #mask_loss.backward()\n",
    "        loss = cls_loss + mask_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()#update parameters\n",
    "        #show loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "#model = model.cpu()#release gpu memory\n",
    "#torch.cuda.empty_cache()\n",
    "\n",
    "#hash code of train data \n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize +1\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    x_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    x_hash, _, _ = best_net(x_batch.permute(0, 3, 1, 2))#forword\n",
    "    trF.extend(x_hash.cpu().data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data\n",
    "teF = [] \n",
    "num_batches = len(teY) // batchSize  +1\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teY), (i+1)*batchSize])\n",
    "    x_batch = torch.from_numpy(np.array(teI[min_idx:max_idx])).type(torch.FloatTensor).cuda()\n",
    "    x_hash, _, _ = best_net(x_batch.permute(0, 3, 1, 2))#forword\n",
    "    teF.extend(x_hash.cpu().data.numpy().tolist()) #record feature\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#evaluate\n",
    "#compute the size of lesion\n",
    "def Func_IOU_size(pred,target):\n",
    "    ious = []\n",
    "    # ignore IOU for background class\n",
    "    pred_inds = pred != 0\n",
    "    pred_sum = pred_inds.sum()\n",
    "    target_inds = target != 0\n",
    "    target_sum = target_inds.sum()\n",
    "    ious.append(round(float(min(pred_sum,target_sum)/max(pred_sum,target_sum)),4))\n",
    "    return np.mean(ious)\n",
    "# buliding index of trainset\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(64) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(trF, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "for topk in [10]:\n",
    "    mAP = [] #mean average precision\n",
    "    mIoU = []\n",
    "    scores, neighbors = gpu_index.search(np.array(teF).astype('float32'), k=topk)\n",
    "    for i, teVal in enumerate(teF):\n",
    "        stype = teY[i]\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        #for j in ranklist:\n",
    "        for j in neighbors[i].tolist():\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                pos_len = pos_len +1\n",
    "                mAP.append(pos_len/rank_len) \n",
    "            else: \n",
    "                mAP.append(0)\n",
    "            mIoU.append(Func_IOU_size(teM[i],trM[j]))\n",
    "    print(\"mAP={:.4f}, mIoU={:.4f}\".format(np.mean(mAP),np.mean(mIoU)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py:2457: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/root/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py:2539: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: 192576, 0.004333496: 3, 0.014831543: 3, 0.015075684: 3, 0.015625: 6, 0.022155762: 3, 0.08929443: 3, 0.08984375: 3, 0.1449585: 3, 0.1642456: 3, 0.17120361: 3, 0.171875: 3, 0.203125: 3, 0.27215576: 3, 0.29901123: 3, 0.36187744: 3, 0.41680908: 3, 0.4208374: 3, 0.421875: 15, 0.45703125: 3, 0.45941162: 3, 0.4673462: 3, 0.4824829: 3, 0.48828125: 3, 0.49945068: 3, 0.5050659: 3, 0.578125: 3, 0.6017456: 3, 0.609375: 3, 0.61505127: 3, 0.66693115: 3, 0.7116089: 3, 0.7296753: 3, 0.734375: 3, 0.765625: 6, 0.77752686: 3, 0.7789917: 3, 0.796875: 3, 0.80133057: 3, 0.82147217: 3, 0.890625: 3, 0.93011475: 3, 0.94366455: 3, 0.953125: 3, 0.96795654: 3, 0.98046875: 3, 0.984375: 18, 0.98480225: 3, 0.98931885: 3, 0.9990845: 3, 1.0: 3852}\n",
      "(256, 256, 3)\n",
      "torch.Size([10, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "def all_np(arr):\n",
    "    arr = np.array(arr)\n",
    "    key = np.unique(arr)\n",
    "    result = {}\n",
    "    for k in key:\n",
    "        mask = (arr == k)\n",
    "        arr_new = arr[mask]\n",
    "        v = arr_new.size\n",
    "        result[k] = v\n",
    "    return result\n",
    "net = NNet(block=Bottleneck)\n",
    "_,_,mask = net(Variable(torch.randn(10,3,256,256)))\n",
    "#for fm in fms:\n",
    "#    print(fm.size())\n",
    "print (all_np(trM[0]))\n",
    "print (trM[0].shape)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 328 / 328 : loss = 1.517603Eopch:     1 mean_loss = 1.701432\n",
      " 328 / 328 : loss = 0.965301Eopch:     2 mean_loss = 1.196696\n",
      " 328 / 328 : loss = 0.352329Eopch:     3 mean_loss = 0.769121\n",
      " 328 / 328 : loss = 0.206907Eopch:     4 mean_loss = 0.512234\n",
      " 328 / 328 : loss = 0.141268Eopch:     5 mean_loss = 0.308589\n",
      " 328 / 328 : loss = 0.325312Eopch:     6 mean_loss = 0.249532\n",
      " 328 / 328 : loss = 0.233006Eopch:     7 mean_loss = 0.197927\n",
      " 328 / 328 : loss = 0.137462Eopch:     8 mean_loss = 0.189368\n",
      " 328 / 328 : loss = 0.269158Eopch:     9 mean_loss = 0.147576\n",
      " 328 / 328 : loss = 0.035677Eopch:    10 mean_loss = 0.158538\n",
      "best_loss = 0.147576\n",
      " 36 / 37 8 Completed buliding index in 22 seconds\n",
      "mAP=0.5996, mIoU=0.4189\n"
     ]
    }
   ],
   "source": [
    "#NNet0\n",
    "#Generate image pairs for model\n",
    "def onlineGenImgPairs( ):\n",
    "    idx_sf = []\n",
    "    idx_0 = np.where( np.array(trY) == 0 ) #class 0\n",
    "    idx_0 = list(idx_0[0])\n",
    "    idx_sf.extend(idx_0)\n",
    "    idx_1 = np.where( np.array(trY) == 1 ) #class 1\n",
    "    idx_1 = list(idx_1[0])\n",
    "    idx_sf.extend(idx_1)\n",
    "    random.shuffle(idx_sf)   \n",
    "    trQ_sf, trP_sf, trN_sf = [], [], []\n",
    "    trQ_y, trP_y, trN_y = [], [], []\n",
    "    trQ_m, trP_m, trN_m = [], [], []\n",
    "    for iQ in idx_sf:\n",
    "        trQ_sf.append(trI[iQ])\n",
    "        trQ_y.append(trY[iQ])\n",
    "        trQ_m.append(trM[iQ])\n",
    "        if trY[iQ] == 0:\n",
    "            idx_tmp = idx_0.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            trP_y.append(trY[iP[0]])\n",
    "            trP_m.append(trM[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_0))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "            trN_y.append(trY[iN[0]])\n",
    "            trN_m.append(trM[iN[0]])\n",
    "        elif trY[iQ] == 1:\n",
    "            idx_tmp = idx_1.copy()\n",
    "            idx_tmp.remove(iQ)\n",
    "            iP =  random.sample(idx_tmp,1) #remove self,then get one positive sample\n",
    "            trP_sf.append(trI[iP[0]])\n",
    "            trP_y.append(trY[iP[0]])\n",
    "            trP_m.append(trM[iP[0]])\n",
    "            idx_sf_tmp = list(set(idx_sf) - set(idx_1))\n",
    "            iN =  random.sample(idx_sf_tmp,1) #remove positive and get one negative sample\n",
    "            trN_sf.append(trI[iN[0]])\n",
    "            trN_y.append(trY[iN[0]])\n",
    "            trN_m.append(trM[iN[0]])\n",
    "        else: pass\n",
    "        sys.stdout.write('\\r{} / {} '.format(len(trQ_sf),len(idx_sf)))\n",
    "        sys.stdout.flush()\n",
    "    return np.array(trQ_sf),np.array(trP_sf),np.array(trN_sf), np.array(trQ_y), np.array(trP_y), np.array(trN_y), np.array(trQ_m), np.array(trP_m), np.array(trN_m)\n",
    "#sample  triplet labels\n",
    "trQ_sf, trP_sf, trN_sf, trQ_y, trP_y, trN_y, trQ_m, trP_m, trN_m = onlineGenImgPairs() \n",
    "assert (trQ_sf.shape==trP_sf.shape and trQ_sf.shape==trN_sf.shape)\n",
    "assert (trQ_y.shape==trP_y.shape and trQ_y.shape==trN_y.shape)\n",
    "assert (trQ_m.shape==trP_m.shape and trQ_m.shape==trN_m.shape)\n",
    "assert (np.mean(np.where((np.array(trQ_y)-np.array(trP_y))!=0,1,0))==0.0)\n",
    "assert (np.mean(np.where((np.array(trQ_y)-np.array(trN_y))!=0,1,0))==1.0)\n",
    "\n",
    "#define model\n",
    "model = NNet(block=Bottleneck).cuda()#initialize model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #define optimizer\n",
    "tl_loss  = TripletLoss(margin=0.5).cuda() #define TripletLoss \n",
    "ce_loss  = nn.CrossEntropyLoss().cuda() #define ce mutli-classes #F.log_softmax+F.nll_loss\n",
    "#train model\n",
    "best_net, best_loss = None, float('inf')\n",
    "batchSize = 10\n",
    "for epoch in range(10):#iteration\n",
    "    losses = []\n",
    "    shuffled_idx = np.random.permutation(np.arange(len(trQ_sf)))\n",
    "    train_q = trQ_sf[shuffled_idx]\n",
    "    train_q_y = trQ_y[shuffled_idx]\n",
    "    train_q_m = trQ_m[shuffled_idx]\n",
    "    train_p = trP_sf[shuffled_idx]\n",
    "    train_p_y = trP_y[shuffled_idx]\n",
    "    train_p_m = trP_m[shuffled_idx]\n",
    "    train_n = trN_sf[shuffled_idx]\n",
    "    train_n_y = trN_y[shuffled_idx]\n",
    "    train_n_m = trN_m[shuffled_idx]\n",
    "    num_batches = len(trQ_sf) // batchSize + 1\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()#grad vanish\n",
    "        min_idx = i * batchSize\n",
    "        max_idx = np.min([len(trQ_sf), (i+1)*batchSize])\n",
    "        Q_batch = torch.from_numpy(train_q[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        Q_y_batch = torch.from_numpy(train_q_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        Q_m_batch = torch.from_numpy(train_q_m[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        P_batch = torch.from_numpy(train_p[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        P_y_batch = torch.from_numpy(train_p_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        P_m_batch = torch.from_numpy(train_p_m[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        N_batch = torch.from_numpy(train_n[min_idx:max_idx]).type(torch.FloatTensor).cuda()\n",
    "        N_y_batch = torch.from_numpy(train_n_y[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        N_m_batch = torch.from_numpy(train_n_m[min_idx:max_idx]).type(torch.LongTensor).cuda()\n",
    "        #forword\n",
    "        Q_hash, Q_cls, Q_mask = model(Q_batch.permute(0, 3, 1, 2))#permute the dims of matrix\n",
    "        P_hash, P_cls, P_mask = model(P_batch.permute(0, 3, 1, 2))\n",
    "        N_hash, N_cls, N_mask = model(N_batch.permute(0, 3, 1, 2))\n",
    "        #loss\n",
    "        #hash_loss = tl_loss(Q_hash,P_hash,N_hash)\n",
    "        cls_loss = ce_loss(Q_cls,Q_y_batch) + ce_loss(P_cls,P_y_batch) + ce_loss(N_cls,N_y_batch)\n",
    "        #cls_loss.backward(retain_graph=True) #buffer\n",
    "        cls_loss.backward()\n",
    "        #mask_loss = ce_loss(Q_mask,Q_m_batch) + ce_loss(P_mask,P_m_batch) + ce_loss(N_mask,N_m_batch)\n",
    "        #mask_loss.backward()\n",
    "        optimizer.step()#update parameters\n",
    "        #show loss\n",
    "        #loss = cls_loss + mask_loss\n",
    "        loss  = cls_loss\n",
    "        sys.stdout.write('\\r {} / {} : loss = {}'.format(i+1, num_batches, float('%0.6f'%loss.item())))\n",
    "        sys.stdout.flush()     \n",
    "        losses.append(loss.item())\n",
    "    print(\"Eopch: %5d mean_loss = %.6f\" % (epoch + 1, np.mean(losses)))\n",
    "    if np.mean(losses) < best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        best_net = copy.deepcopy(model)\n",
    "print(\"best_loss = %.6f\" % (best_loss))\n",
    "model = model.cpu()#release gpu memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#hash code of train data \n",
    "batchSize = 10\n",
    "num_batches = len(trI) // batchSize +1\n",
    "trF = []\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(trI), (i+1)*batchSize])\n",
    "    x_batch = torch.from_numpy(np.array(trI[min_idx: max_idx])).type(torch.FloatTensor).cuda()\n",
    "    x_hash, _, _ = best_net(x_batch.permute(0, 3, 1, 2))#forword\n",
    "    trF.extend(x_hash.cpu().data.numpy().tolist())\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#hash code of test data\n",
    "teF = [] \n",
    "num_batches = len(teY) // batchSize  +1\n",
    "for i in range(num_batches):\n",
    "    min_idx = i * batchSize\n",
    "    max_idx = np.min([len(teY), (i+1)*batchSize])\n",
    "    x_batch = torch.from_numpy(np.array(teI[min_idx:max_idx])).type(torch.FloatTensor).cuda()\n",
    "    x_hash, _, _ = best_net(x_batch.permute(0, 3, 1, 2))#forword\n",
    "    teF.extend(x_hash.cpu().data.numpy().tolist()) #record feature\n",
    "    sys.stdout.write('\\r {} / {} '.format(i, num_batches))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#evaluate\n",
    "#compute the size of lesion\n",
    "def Func_IOU_size(pred,target):\n",
    "    ious = []\n",
    "    # ignore IOU for background class\n",
    "    pred_inds = pred != 0\n",
    "    pred_sum = pred_inds.sum()\n",
    "    target_inds = target != 0\n",
    "    target_sum = target_inds.sum()\n",
    "    ious.append(round(float(min(pred_sum,target_sum)/max(pred_sum,target_sum)),4))\n",
    "    return np.mean(ious)\n",
    "# buliding index of trainset\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(64) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(trF, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "for topk in [10]:\n",
    "    mAP = [] #mean average precision\n",
    "    mIoU = []\n",
    "    scores, neighbors = gpu_index.search(np.array(teF).astype('float32'), k=topk)\n",
    "    for i, teVal in enumerate(teF):\n",
    "        stype = teY[i]\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        #for j in ranklist:\n",
    "        for j in neighbors[i].tolist():\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                pos_len = pos_len +1\n",
    "                mAP.append(pos_len/rank_len) \n",
    "            else: \n",
    "                mAP.append(0)\n",
    "            mIoU.append(Func_IOU_size(teM[i],trM[j]))\n",
    "    print(\"mAP={:.4f}, mIoU={:.4f}\".format(np.mean(mAP),np.mean(mIoU)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
