{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading faiss with AVX2 support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import math\n",
    "import random\n",
    "import heapq \n",
    "import time\n",
    "import copy\n",
    "import itertools  \n",
    "from typing import Dict, List\n",
    "from PIL import Image\n",
    "from io import StringIO,BytesIO \n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.signal import butter, lfilter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import confusion_matrix,roc_curve,accuracy_score,auc,roc_auc_score \n",
    "from sklearn.decomposition import PCA\n",
    "from functools import reduce\n",
    "from scipy.io import loadmat\n",
    "import cv2\n",
    "import faiss \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.ops as ops\n",
    "torch.cuda.set_device(7)\n",
    "print (torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1 / 3279 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py:2539: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 / 3279 1216.PNG:/data/fjsdata/MCBIR-Ins/TNSCUI2020_train/image/1216.PNG\n",
      "264 / 3279 4662.PNG:/data/fjsdata/MCBIR-Ins/TNSCUI2020_train/image/4662.PNG\n",
      "398 / 3279 1309.PNG:/data/fjsdata/MCBIR-Ins/TNSCUI2020_train/image/1309.PNG\n",
      "400 / 3279 1473.PNG:/data/fjsdata/MCBIR-Ins/TNSCUI2020_train/image/1473.PNG\n",
      "448 / 3279 4501.PNG:/data/fjsdata/MCBIR-Ins/TNSCUI2020_train/image/4501.PNG\n",
      "524 / 3279 2558.PNG:/data/fjsdata/MCBIR-Ins/TNSCUI2020_train/image/2558.PNG\n",
      "532 / 3279 4432.PNG:/data/fjsdata/MCBIR-Ins/TNSCUI2020_train/image/4432.PNG\n",
      "582 / 3279 53.PNG:/data/fjsdata/MCBIR-Ins/TNSCUI2020_train/image/53.PNG\n",
      "770 / 3279 1786.PNG:/data/fjsdata/MCBIR-Ins/TNSCUI2020_train/image/1786.PNG\n",
      "814 / 3279 1306.PNG:/data/fjsdata/MCBIR-Ins/TNSCUI2020_train/image/1306.PNG\n",
      "994 / 3279 631.PNG:/data/fjsdata/MCBIR-Ins/TNSCUI2020_train/image/631.PNG\n",
      "1227 / 3279 4596.PNG:/data/fjsdata/MCBIR-Ins/TNSCUI2020_train/image/4596.PNG\n",
      "1347 / 3279 1339.PNG:/data/fjsdata/MCBIR-Ins/TNSCUI2020_train/image/1339.PNG\n",
      "1402 / 3279 4624.PNG:/data/fjsdata/MCBIR-Ins/TNSCUI2020_train/image/4624.PNG\n",
      "1452 / 3279 4591.PNG:/data/fjsdata/MCBIR-Ins/TNSCUI2020_train/image/4591.PNG\n",
      "1543 / 3279 7258.PNG:/data/fjsdata/MCBIR-Ins/TNSCUI2020_train/image/7258.PNG\n",
      "1741 / 3279 1331.PNG:/data/fjsdata/MCBIR-Ins/TNSCUI2020_train/image/1331.PNG\n",
      "1818 / 3279 2587.PNG:/data/fjsdata/MCBIR-Ins/TNSCUI2020_train/image/2587.PNG\n",
      "2429 / 3279 4534.PNG:/data/fjsdata/MCBIR-Ins/TNSCUI2020_train/image/4534.PNG\n",
      "2544 / 3279 5603.PNG:/data/fjsdata/MCBIR-Ins/TNSCUI2020_train/image/5603.PNG\n",
      "2851 / 3279 4538.PNG:/data/fjsdata/MCBIR-Ins/TNSCUI2020_train/image/4538.PNG\n",
      "3279 / 3279 The length of trainset is 3279\n",
      "94 / 365 7287.PNG:/data/fjsdata/MCBIR-Ins/TNSCUI2020_train/image/7287.PNG\n",
      "109 / 365 4526.PNG:/data/fjsdata/MCBIR-Ins/TNSCUI2020_train/image/4526.PNG\n",
      "265 / 365 57.PNG:/data/fjsdata/MCBIR-Ins/TNSCUI2020_train/image/57.PNG\n",
      "277 / 365 4554.PNG:/data/fjsdata/MCBIR-Ins/TNSCUI2020_train/image/4554.PNG\n",
      "365 / 365 The length of testset is 365\n",
      "Completed data handle in 640 seconds\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Code: https://github.com/imatge-upc/retrieval-2016-deepvision\n",
    "       https://github.com/WXinlong/SOLO/blob/master/mmdet/models/detectors/faster_rcnn.py \n",
    "#Ref:\n",
    "       https://github.com/jwyang/faster-rcnn.pytorch\n",
    "       https://github.com/Liu-Yicheng/Faster-rcnn\n",
    "       R-FCN：https://github.com/princewang1994/RFCN_CoupleNet.pytorch\n",
    "       YOLOv3：https://github.com/eriklindernoren/PyTorch-YOLOv3\n",
    "       SSD：https://github.com/amdegroot/ssd.pytorch\n",
    "#Paper: CVPR2016《Faster r-cnn features for instance search》\n",
    "'''\n",
    "import sys;\n",
    "sys.path.append(\"/data/tmpexec/VOC2007/SOLO\")\n",
    "from mmdet.apis import init_detector, inference_detector, show_result_pyplot, show_result_ins\n",
    "import mmcv\n",
    "\n",
    "def pool_feats(feat, pooling='max'):\n",
    "    feat = np.expand_dims(feat,axis=0)\n",
    "    if pooling is 'max':\n",
    "        feat = np.max(np.max(feat,axis=2),axis=1)\n",
    "    else:\n",
    "        feat = np.sum(np.sum(feat,axis=2),axis=1)\n",
    "    return feat\n",
    "\n",
    "config_file = '/data/tmpexec/VOC2007/SOLO/configs/solo/decoupled_solo_r50_fpn_8gpu_3x.py'\n",
    "# download the checkpoint from model zoo and put it in `checkpoints/`\n",
    "checkpoint_file = '/data/tmpexec/VOC2007/SOLO/checkpoints/DECOUPLED_SOLO_R50_3x.pth'\n",
    "# build the model from a config file and a checkpoint file\n",
    "model = init_detector(config_file, checkpoint_file, device='cuda:0')\n",
    "\n",
    "tstart = time.time()\n",
    "root_dir = '/data/fjsdata/MCBIR-Ins/TNSCUI2020_train/' #the path of images\n",
    "trData = pd.read_csv(root_dir+\"trainset.csv\" , sep=',')\n",
    "teData = pd.read_csv(root_dir+\"testset.csv\" , sep=',')\n",
    "#trainset \n",
    "trN, trM, trY, trF = [],[],[],[]\n",
    "for iname, itype in np.array(trData).tolist():\n",
    "    try:\n",
    "        trN.append(iname)\n",
    "        trY.append(itype) #0 refer to Benign, and 1 refers to malignant\n",
    "        image_path = os.path.join(root_dir, 'image', iname)\n",
    "        result = inference_detector(model, image_path)\n",
    "        result = result[0][0].cpu().data.numpy()\n",
    "        feat = pool_feats(result)\n",
    "        trF.append(feat.flatten()[0:128])\n",
    "        mask_path = os.path.join(root_dir,'mask', iname)\n",
    "        mask = cv2.resize(cv2.imread(mask_path).astype(np.float32), (256, 256))#(256,256)\n",
    "        trM.append(mask)\n",
    "    except:\n",
    "        print(iname+\":\"+str(image_path))\n",
    "    sys.stdout.write('\\r{} / {} '.format(len(trN),trData.shape[0]))\n",
    "    sys.stdout.flush()\n",
    "print('The length of trainset is %d'%len(trN))\n",
    "#testset\n",
    "teN, teM, teY, teF = [],[],[],[]\n",
    "for iname, itype in np.array(teData).tolist():\n",
    "    try:\n",
    "        teN.append(iname)\n",
    "        teY.append(itype) #0 refer to Benign, and 1 refers to malignant\n",
    "        image_path = os.path.join(root_dir, 'image', iname)\n",
    "        result = inference_detector(model, image_path)\n",
    "        result = result[0][0].cpu().data.numpy()\n",
    "        feat = pool_feats(result)\n",
    "        teF.append(feat.flatten()[0:128])\n",
    "        mask_path = os.path.join(root_dir,'mask', iname)\n",
    "        mask = cv2.resize(cv2.imread(mask_path).astype(np.float32), (256, 256))#(256,256)\n",
    "        teM.append(mask)\n",
    "    except:\n",
    "        print(iname+\":\"+str(image_path))\n",
    "    sys.stdout.write('\\r{} / {} '.format(len(teN),teData.shape[0]))\n",
    "    sys.stdout.flush()\n",
    "print('The length of testset is %d'%len(teN))\n",
    "\n",
    "elapsed = time.time() - tstart\n",
    "print('Completed data handle in %d seconds' % int(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3258, 128)\n",
      "(361, 128)\n",
      "(1, 856)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(trF).shape)\n",
    "print(np.array(teF).shape)\n",
    "print(feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed buliding index in 21 seconds\n",
      "mAP=0.3996, mIoU=0.3935\n",
      "mAP=0.3658, mIoU=0.4013\n",
      "mAP=0.3555, mIoU=0.3925\n"
     ]
    }
   ],
   "source": [
    "#compute the size of lesion\n",
    "def Func_IOU_size(pred,target):\n",
    "    ious = []\n",
    "    # ignore IOU for background class\n",
    "    pred_inds = pred != 0\n",
    "    pred_sum = pred_inds.sum()\n",
    "    target_inds = target != 0\n",
    "    target_sum = target_inds.sum()\n",
    "    ious.append(round(float(min(pred_sum,target_sum)/max(pred_sum,target_sum)),4))\n",
    "    return np.mean(ious)\n",
    "# buliding index of trainset\n",
    "tstart = time.time()\n",
    "cpu_index = faiss.IndexFlatL2(128) #\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index) #make all gpu usable\n",
    "gpu_index.add(np.ascontiguousarray(trF, dtype=np.float32)) #add data(must be float32) to index\n",
    "elapsed = time.time() - tstart    \n",
    "print('Completed buliding index in %d seconds' % int(elapsed))\n",
    "for topk in [5, 10, 20, 50]:\n",
    "    mAP = [] #mean average precision\n",
    "    mIoU = []\n",
    "    scores, neighbors = gpu_index.search(np.array(teF).astype('float32'), k=topk)\n",
    "    for i, teVal in enumerate(teF):\n",
    "        stype = teY[i]\n",
    "        #perfromance\n",
    "        pos_len = 0\n",
    "        rank_len = 0\n",
    "        #for j in ranklist:\n",
    "        for j in neighbors[i].tolist():\n",
    "            dtype = trY[j]\n",
    "            rank_len=rank_len+1\n",
    "            if stype==dtype:  #hit\n",
    "                pos_len = pos_len +1\n",
    "                mAP.append(pos_len/rank_len) \n",
    "            else: \n",
    "                mAP.append(0)\n",
    "            mIoU.append(Func_IOU_size(teM[i],trM[j]))\n",
    "    print(\"mAP={:.4f}, mIoU={:.4f}\".format(np.mean(mAP),np.mean(mIoU)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "--------below is the dataset split code and image show code--------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=1, checkpoint_model=None, class_path='/data/tmpexec/PyTorch-YOLOv3/data/coco.names', conf_thres=0.8, image_folder='/data/tmpexec/PyTorch-YOLOv3/data/samples', img_size=416, model_def='/data/tmpexec/PyTorch-YOLOv3/config/yolov3.cfg', n_cpu=0, nms_thres=0.4, weights_path='/data/tmpexec/PyTorch-YOLOv3/weights/yolov3.weights')\n",
      "\n",
      "Performing object detection:\n",
      "torch.Size([1, 3, 416, 416])\n",
      "torch.Size([1, 10647, 85])\n",
      "\t+ Batch 0, Inference Time: 0:00:00.053645\n",
      "torch.Size([1, 3, 416, 416])\n",
      "torch.Size([1, 10647, 85])\n",
      "\t+ Batch 1, Inference Time: 0:00:00.027473\n",
      "torch.Size([1, 3, 416, 416])\n",
      "torch.Size([1, 10647, 85])\n",
      "\t+ Batch 2, Inference Time: 0:00:00.025320\n",
      "torch.Size([1, 3, 416, 416])\n",
      "torch.Size([1, 10647, 85])\n",
      "\t+ Batch 3, Inference Time: 0:00:00.131455\n",
      "torch.Size([1, 3, 416, 416])\n",
      "torch.Size([1, 10647, 85])\n",
      "\t+ Batch 4, Inference Time: 0:00:00.030438\n",
      "torch.Size([1, 3, 416, 416])\n",
      "torch.Size([1, 10647, 85])\n",
      "\t+ Batch 5, Inference Time: 0:00:00.027984\n",
      "torch.Size([1, 3, 416, 416])\n",
      "torch.Size([1, 10647, 85])\n",
      "\t+ Batch 6, Inference Time: 0:00:00.035148\n",
      "torch.Size([1, 3, 416, 416])\n",
      "torch.Size([1, 10647, 85])\n",
      "\t+ Batch 7, Inference Time: 0:00:00.022840\n",
      "torch.Size([1, 3, 416, 416])\n",
      "torch.Size([1, 10647, 85])\n",
      "\t+ Batch 8, Inference Time: 0:00:00.023366\n",
      "torch.Size([1, 3, 416, 416])\n",
      "torch.Size([1, 10647, 85])\n",
      "\t+ Batch 9, Inference Time: 0:00:00.026850\n",
      "\n",
      "Saving images:\n",
      "(0) Image: '/data/tmpexec/PyTorch-YOLOv3/data/samples/dog.jpg'\n",
      "\t+ Label: dog, Conf: 0.99335\n",
      "\t+ Label: bicycle, Conf: 0.99981\n",
      "\t+ Label: truck, Conf: 0.94229\n",
      "(1) Image: '/data/tmpexec/PyTorch-YOLOv3/data/samples/eagle.jpg'\n",
      "\t+ Label: bird, Conf: 0.99703\n",
      "(2) Image: '/data/tmpexec/PyTorch-YOLOv3/data/samples/field.jpg'\n",
      "\t+ Label: person, Conf: 0.99996\n",
      "\t+ Label: horse, Conf: 0.99977\n",
      "\t+ Label: dog, Conf: 0.99409\n",
      "(3) Image: '/data/tmpexec/PyTorch-YOLOv3/data/samples/fundus001.jpg'\n",
      "\t+ Label: diningtable, Conf: 0.11312\n",
      "(4) Image: '/data/tmpexec/PyTorch-YOLOv3/data/samples/giraffe.jpg'\n",
      "\t+ Label: giraffe, Conf: 0.99959\n",
      "\t+ Label: zebra, Conf: 0.97958\n",
      "(5) Image: '/data/tmpexec/PyTorch-YOLOv3/data/samples/herd_of_horses.jpg'\n",
      "\t+ Label: horse, Conf: 0.99459\n",
      "\t+ Label: horse, Conf: 0.99352\n",
      "\t+ Label: horse, Conf: 0.96845\n",
      "\t+ Label: horse, Conf: 0.99478\n",
      "(6) Image: '/data/tmpexec/PyTorch-YOLOv3/data/samples/messi.jpg'\n",
      "\t+ Label: person, Conf: 0.99993\n",
      "\t+ Label: person, Conf: 0.99984\n",
      "\t+ Label: person, Conf: 0.99996\n",
      "(7) Image: '/data/tmpexec/PyTorch-YOLOv3/data/samples/person.jpg'\n",
      "\t+ Label: person, Conf: 0.99883\n",
      "\t+ Label: dog, Conf: 0.99275\n",
      "(8) Image: '/data/tmpexec/PyTorch-YOLOv3/data/samples/room.jpg'\n",
      "\t+ Label: chair, Conf: 0.99906\n",
      "\t+ Label: chair, Conf: 0.96942\n",
      "\t+ Label: clock, Conf: 0.99971\n",
      "(9) Image: '/data/tmpexec/PyTorch-YOLOv3/data/samples/street.jpg'\n",
      "\t+ Label: car, Conf: 0.99977\n",
      "\t+ Label: car, Conf: 0.99402\n",
      "\t+ Label: car, Conf: 0.99841\n",
      "\t+ Label: car, Conf: 0.99785\n",
      "\t+ Label: car, Conf: 0.97907\n",
      "\t+ Label: car, Conf: 0.95370\n",
      "\t+ Label: traffic light, Conf: 0.99995\n",
      "\t+ Label: car, Conf: 0.62254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#https://hellozhaozheng.github.io/z_post/PyTorch-YOLO/\n",
    "from __future__ import division\n",
    "import sys;\n",
    "sys.path.append(\"/data/tmpexec/PyTorch-YOLOv3\")\n",
    "\n",
    "from models import *\n",
    "from utils.utils import *\n",
    "from utils.datasets import *\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import argparse\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.ticker import NullLocator\n",
    "\n",
    "import argparse\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--image_folder\", type=str, default=\"/data/tmpexec/PyTorch-YOLOv3/data/samples\", help=\"path to dataset\")\n",
    "    parser.add_argument(\"--model_def\", type=str, default=\"/data/tmpexec/PyTorch-YOLOv3/config/yolov3.cfg\", help=\"path to model definition file\")\n",
    "    parser.add_argument(\"--weights_path\", type=str, default=\"/data/tmpexec/PyTorch-YOLOv3/weights/yolov3.weights\", help=\"path to weights file\")\n",
    "    parser.add_argument(\"--class_path\", type=str, default=\"/data/tmpexec/PyTorch-YOLOv3/data/coco.names\", help=\"path to class label file\")\n",
    "    parser.add_argument(\"--conf_thres\", type=float, default=0.8, help=\"object confidence threshold\")\n",
    "    parser.add_argument(\"--nms_thres\", type=float, default=0.4, help=\"iou thresshold for non-maximum suppression\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1, help=\"size of the batches\")\n",
    "    parser.add_argument(\"--n_cpu\", type=int, default=0, help=\"number of cpu threads to use during batch generation\")\n",
    "    parser.add_argument(\"--img_size\", type=int, default=416, help=\"size of each image dimension\")\n",
    "    parser.add_argument(\"--checkpoint_model\", type=str, help=\"path to checkpoint model\")\n",
    "    opt = parser.parse_args(args=[])\n",
    "    #opt = parser.parse_args()\n",
    "    print(opt)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    #os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "    # Set up model\n",
    "    model = Darknet(opt.model_def, img_size=opt.img_size).to(device)\n",
    "\n",
    "    if opt.weights_path.endswith(\".weights\"):\n",
    "        # Load darknet weights\n",
    "        model.load_darknet_weights(opt.weights_path)\n",
    "    else:\n",
    "        # Load checkpoint weights\n",
    "        model.load_state_dict(torch.load(opt.weights_path))\n",
    "\n",
    "    model.eval()  # Set in evaluation mode\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        ImageFolder(opt.image_folder, img_size=opt.img_size),\n",
    "        batch_size=opt.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=opt.n_cpu,\n",
    "    )\n",
    "\n",
    "    classes = load_classes(opt.class_path)  # Extracts class labels from file\n",
    "\n",
    "    Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "\n",
    "    imgs = []  # Stores image paths\n",
    "    img_detections = []  # Stores detections for each image index\n",
    "\n",
    "    print(\"\\nPerforming object detection:\")\n",
    "    prev_time = time.time()\n",
    "    for batch_i, (img_paths, input_imgs) in enumerate(dataloader):\n",
    "        # Configure input\n",
    "        input_imgs = Variable(input_imgs.type(Tensor))\n",
    "        print(input_imgs.shape)\n",
    "\n",
    "        # Get detections\n",
    "        with torch.no_grad():\n",
    "            detections = model(input_imgs)\n",
    "            #fdfd = pool_feats(detections)\n",
    "            #print(detections.shape)\n",
    "             # detections的shape为: [1,10647,85], 其中, 1为batch_size\n",
    "            # 因为对于尺寸为416的图片来说:(13*13+26*26+52*52) * 3 = 10647\n",
    "            # 如果图片尺寸为608(必须为32的倍数), 那么就为:(19*19+38*38+76*76) * 3 = 22743\n",
    "            detections = non_max_suppression(detections, opt.conf_thres, opt.nms_thres)\n",
    "\n",
    "        # Log progress\n",
    "        current_time = time.time()\n",
    "        inference_time = datetime.timedelta(seconds=current_time - prev_time)\n",
    "        prev_time = current_time\n",
    "        print(\"\\t+ Batch %d, Inference Time: %s\" % (batch_i, inference_time))\n",
    "\n",
    "        # Save image and detections\n",
    "        imgs.extend(img_paths)\n",
    "        img_detections.extend(detections)\n",
    "\n",
    "    # Bounding-box colors\n",
    "    cmap = plt.get_cmap(\"tab20b\")\n",
    "    colors = [cmap(i) for i in np.linspace(0, 1, 20)]\n",
    "\n",
    "    print(\"\\nSaving images:\")\n",
    "    # Iterate through images and save plot of detections\n",
    "    for img_i, (path, detections) in enumerate(zip(imgs, img_detections)):\n",
    "\n",
    "        print(\"(%d) Image: '%s'\" % (img_i, path))\n",
    "\n",
    "        # Create plot\n",
    "        img = np.array(Image.open(path))\n",
    "        plt.figure()\n",
    "        fig, ax = plt.subplots(1)\n",
    "        ax.imshow(img)\n",
    "\n",
    "        # Draw bounding boxes and labels of detections\n",
    "        if detections is not None:\n",
    "            # Rescale boxes to original image\n",
    "            detections = rescale_boxes(detections, opt.img_size, img.shape[:2])\n",
    "            unique_labels = detections[:, -1].cpu().unique()\n",
    "            n_cls_preds = len(unique_labels)\n",
    "            bbox_colors = random.sample(colors, n_cls_preds)\n",
    "            for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections:\n",
    "\n",
    "                print(\"\\t+ Label: %s, Conf: %.5f\" % (classes[int(cls_pred)], cls_conf.item()))\n",
    "\n",
    "                box_w = x2 - x1\n",
    "                box_h = y2 - y1\n",
    "\n",
    "                color = bbox_colors[int(np.where(unique_labels == int(cls_pred))[0])]\n",
    "                # Create a Rectangle patch\n",
    "                bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=2, edgecolor=color, facecolor=\"none\")\n",
    "                # Add the bbox to the plot\n",
    "                ax.add_patch(bbox)\n",
    "                # Add label\n",
    "                plt.text(\n",
    "                    x1,\n",
    "                    y1,\n",
    "                    s=classes[int(cls_pred)],\n",
    "                    color=\"white\",\n",
    "                    verticalalignment=\"top\",\n",
    "                    bbox={\"color\": color, \"pad\": 0},\n",
    "                )\n",
    "\n",
    "        # Save generated image with detections\n",
    "        plt.axis(\"off\")\n",
    "        plt.gca().xaxis.set_major_locator(NullLocator())\n",
    "        plt.gca().yaxis.set_major_locator(NullLocator())\n",
    "        filename = path.split(\"/\")[-1].split(\".\")[0]\n",
    "        plt.savefig(f\"/data/tmpexec/PyTorch-YOLOv3/output/{filename}.png\", bbox_inches=\"tight\", pad_inches=0.0)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys;\n",
    "sys.path.append(\"/data/tmpexec/VOC2007/SOLO\")\n",
    "from mmdet.apis import init_detector, inference_detector, show_result_pyplot, show_result_ins\n",
    "import mmcv\n",
    "\n",
    "\n",
    "config_file = '/data/tmpexec/VOC2007/SOLO/configs/solo/decoupled_solo_r50_fpn_8gpu_3x.py'\n",
    "# download the checkpoint from model zoo and put it in `checkpoints/`\n",
    "checkpoint_file = '/data/tmpexec/VOC2007/SOLO/checkpoints/DECOUPLED_SOLO_R50_3x.pth'\n",
    "\n",
    "# build the model from a config file and a checkpoint file\n",
    "model = init_detector(config_file, checkpoint_file, device='cuda:0')\n",
    "\n",
    "# test a single image\n",
    "img = '/data/fjsdata/MCBIR-Ins/TNSCUI2020_train/image/1216.PNG'\n",
    "result = inference_detector(model, img)\n",
    "\n",
    "\n",
    "#show_result_ins(img, result, model.CLASSES, score_thr=0.25, out_file=\"/data/tmpexec/VOC2007/SOLO/checkpoints/demo_out.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a39b69e37449>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpool_feats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpooling\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;34m'max'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "print(result[0][0].shape)\n",
    "print(result[0][1].shape)\n",
    "print(result[0][2].shape)\n",
    "def pool_feats(feat, pooling='max'):\n",
    "    if pooling is 'max':\n",
    "        feat = np.max(np.max(feat,axis=2),axis=1)\n",
    "    else:\n",
    "        feat = np.sum(np.sum(feat,axis=2),axis=1)\n",
    "    return feat\n",
    "\n",
    "feat = np.expand_dims(result[0][0].cpu(),axis=0)\n",
    "print(pool_feats(feat).flatten().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (167) must match the size of tensor b (168) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-6acd244f4757>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_feat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/tmpexec/VOC2007/SOLO/mmdet/models/detectors/single_stage_ins.py\u001b[0m in \u001b[0;36mextract_feat\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_neck\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/tmpexec/VOC2007/SOLO/mmdet/core/fp16/decorators.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m                                 'method of nn.Module')\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fp16_enabled'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16_enabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mold_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0;31m# get the arg spec of the decorated method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0margs_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/tmpexec/VOC2007/SOLO/mmdet/models/necks/fpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mused_backbone_levels\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             laterals[i - 1] += F.interpolate(\n\u001b[0;32m--> 115\u001b[0;31m                 laterals[i], scale_factor=2, mode='nearest')\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# build outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (167) must match the size of tensor b (168) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "img = cv2.resize(cv2.imread( '/data/tmpexec/VOC2007/SOLO/checkpoints/street.jpg').astype(np.float32), (1333, 800))\n",
    "img = np.expand_dims(img,axis=0)\n",
    "img = torch.from_numpy(img).type(torch.FloatTensor).cuda()\n",
    "feat = model.extract_feat(img.permute(0, 3, 1, 2))\n",
    "print(feat.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
